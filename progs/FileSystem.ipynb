{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# сканирование, и подсчет хешей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T06:37:01.838881Z",
     "start_time": "2019-11-13T06:37:01.827880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверка на симлинки\n",
    "# стандартная проверка в windows junction-ы не считает симлинками\n",
    "\n",
    "from ctypes import *\n",
    "from ctypes.wintypes import *\n",
    "\n",
    "FILE_ATTRIBUTE_REPARSE_POINT = 0x00400\n",
    "INVALID_FILE_ATTRIBUTES = 0xFFFFFFFF\n",
    "\n",
    "kernel32 = WinDLL('kernel32')\n",
    "GetFileAttributesW = kernel32.GetFileAttributesW\n",
    "GetFileAttributesW.restype = DWORD\n",
    "GetFileAttributesW.argtypes = (LPCWSTR,) #lpFileName In\n",
    "\n",
    "def is_winlink(path):\n",
    "    result = GetFileAttributesW(path)\n",
    "    if result == INVALID_FILE_ATTRIBUTES:\n",
    "        raise OSError((path,WinError()))\n",
    "    return bool(result & FILE_ATTRIBUTE_REPARSE_POINT)\n",
    "\n",
    "is_winlink(r'D:\\Users\\feelus\\Local Settings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T06:37:03.330966Z",
     "start_time": "2019-11-13T06:37:02.274906Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce330119e7e4247a712f679418f0e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# прогресс-бар числом - для случаев while и т.п.\n",
    "from ipywidgets import HTML\n",
    "from IPython.display import display\n",
    "from time import sleep\n",
    "label = HTML()\n",
    "display(label)\n",
    "for x in range(10):\n",
    "    label.value = str(x)\n",
    "    sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T06:37:03.348967Z",
     "start_time": "2019-11-13T06:37:03.332967Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a:\\\\c\\\\b'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from stat import *\n",
    "\n",
    "def slash_replacer(s):\n",
    "    while s[0]==os.sep:\n",
    "        s = s[1:]\n",
    "    while s[-1]==os.sep:\n",
    "        s = s[:-1]\n",
    "    return s\n",
    "def my_path_join_a(*ll):\n",
    "    return os.sep.join([slash_replacer(s) for s in ll])\n",
    "def my_path_join_l(ll):\n",
    "    return os.sep.join([slash_replacer(s) for s in ll])\n",
    "my_path_join_a('a:\\\\c\\\\','b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T06:37:03.588981Z",
     "start_time": "2019-11-13T06:37:03.572980Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def scan(rootpath,exceptions=set()):\n",
    "    total_size = 0\n",
    "    ts_printed = 0\n",
    "\n",
    "    label = HTML()\n",
    "    display(label)\n",
    "\n",
    "    def scan1(curpath):\n",
    "        nonlocal total_size\n",
    "        nonlocal ts_printed\n",
    "        if curpath in exceptions:\n",
    "            return {}\n",
    "        curroot = {}\n",
    "        #print(curpath)\n",
    "        try:\n",
    "            with os.scandir(curpath) as it:\n",
    "                for entry in it:\n",
    "                    if entry.name!='.' and entry.name!='..':\n",
    "                        if entry.is_dir(follow_symlinks=False):\n",
    "                            curroot[entry.name] = scan1(entry.path)\n",
    "                        elif entry.is_file(follow_symlinks=False):\n",
    "                            st = entry.stat(follow_symlinks=False)\n",
    "                            curroot[entry.name] = [st.st_mtime,st.st_size]\n",
    "                            total_size+=st.st_size\n",
    "                        elif not entry.is_symlink() and not is_winlink(entry.path):\n",
    "                            print('unknown object:',entry.path)\n",
    "        except OSError as e:\n",
    "            print(curpath)\n",
    "            print(e)\n",
    "            print()\n",
    "            return {}\n",
    "        if ts_printed<int(total_size/1024/1024/1024):\n",
    "            ts_printed = int(total_size/1024/1024/1024)\n",
    "            label.value = str(ts_printed)+' GB scanned'\n",
    "        return curroot\n",
    "    tmp = scan1(rootpath)\n",
    "    label.value = str(ts_printed)+' GB scanned - completed'\n",
    "    return tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T06:37:04.156014Z",
     "start_time": "2019-11-13T06:37:04.141013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('a',), 1)\n",
      "(('b', 'c'), 2)\n",
      "(('b', 'd'), 3)\n"
     ]
    }
   ],
   "source": [
    "def tree_iterator(tree):\n",
    "    \"\"\"проходится по всему дереву\n",
    "    на каждом узле(листе) возвращает пару (путь, значение)\n",
    "    где путь - список имен, по которым надо добираться по дереву до значения\"\"\"\n",
    "    if type(tree)!=dict:\n",
    "        yield (),tree\n",
    "        return\n",
    "    for k,v in tree.items():\n",
    "        if type(v)==dict:\n",
    "            for path,v2 in tree_iterator(v):\n",
    "                #path.insert(0,k)\n",
    "                yield (k,)+path,v2\n",
    "        else:\n",
    "            yield (k,),v\n",
    "r = {'a':1,'b':{'c':2,'d':3}}\n",
    "for x in tree_iterator(r):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T06:37:04.706045Z",
     "start_time": "2019-11-13T06:37:04.695044Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_subtree(root,path):\n",
    "    \"\"\"берет корень и путь, проходит по пути, и возвращает то, где оказался\"\"\"\n",
    "    if len(path)==0: return root\n",
    "    else: return get_subtree(root[path[0]],path[1:])\n",
    "def make_subtree(root,path):\n",
    "    \"\"\"берет корень и путь, проходит по пути и создает его, если его нет\"\"\"\n",
    "    if len(path)==0: return root\n",
    "    else: \n",
    "        if path[0] not in root: root[path[0]]={}\n",
    "        return make_subtree(root[path[0]],path[1:])\n",
    "def is_subpath(subpath,path):\n",
    "    \"\"\"сначала длинный, потом короткий\"\"\"\n",
    "    if len(subpath)<len(path):\n",
    "        return False\n",
    "    for i in range(len(path)):\n",
    "        if subpath[i]!=path[i]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T06:37:05.290078Z",
     "start_time": "2019-11-13T06:37:05.284078Z"
    }
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def md5(fname):\n",
    "    \"\"\"вычисляет хеш файла по его пути\"\"\"\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(fname, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T06:37:05.848110Z",
     "start_time": "2019-11-13T06:37:05.822109Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_hashes(root,old_root,prefix):\n",
    "    total_size = 0\n",
    "    ts_printed = 0\n",
    "    label = HTML()\n",
    "    display(label)\n",
    "\n",
    "    def calc_hashes1(root,old_root,path):\n",
    "        nonlocal total_size\n",
    "        nonlocal ts_printed\n",
    "        \n",
    "        for name in root.keys():\n",
    "            if type(root[name])==dict: # directory\n",
    "                if name in old_root and type(old_root[name])==dict:\n",
    "                    calc_hashes1(root[name],old_root[name],path+[name])\n",
    "                else:\n",
    "                    calc_hashes1(root[name],{},path+[name])\n",
    "            elif type(root[name])==list: # file\n",
    "                assert len(root[name])>=2\n",
    "                if name in old_root and type(old_root[name])==list and \\\n",
    "                  len(old_root[name])>=3 and \\\n",
    "                  root[name][1]==old_root[name][1] and \\\n",
    "                  abs(root[name][0] - old_root[name][0]) < 1 : # 1 second between timestamps\n",
    "                    if len(root[name])==2:\n",
    "                        root[name].append(old_root[name][2])\n",
    "                    else:\n",
    "                        root[name][2] = old_root[name][2]\n",
    "                else:\n",
    "                    p = my_path_join_a(prefix,*path,name)\n",
    "                    try:\n",
    "                        #print(p)\n",
    "                        os.stat(p)  # зависает при чтении некоторых файлов\n",
    "                                    # stat от этих файлов будет ошибкой\n",
    "                        if len(root[name])==2:\n",
    "                            root[name].append(md5(p))\n",
    "                        else:\n",
    "                            root[name][2] = md5(p)\n",
    "                    except OSError as e:\n",
    "                        if len(root[name])==2:\n",
    "                            root[name].append(None)\n",
    "                        print(p)\n",
    "                        print(e)\n",
    "                        print()\n",
    "                    \n",
    "                if type(root[name][1])==int:\n",
    "                    total_size+=root[name][1]\n",
    "                if ts_printed<int(total_size/1024/1024/1024):\n",
    "                    ts_printed = int(total_size/1024/1024/1024)\n",
    "                    label.value = str(ts_printed)+' GB calculated'\n",
    "            else:\n",
    "                raise BaseException(path)\n",
    "                \n",
    "    calc_hashes1(root,old_root,[])\n",
    "    label.value = str(ts_printed)+' GB calculated - completed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T06:37:06.335138Z",
     "start_time": "2019-11-13T06:37:06.322138Z"
    }
   },
   "outputs": [],
   "source": [
    "import json, codecs\n",
    "\n",
    "def myjson_load(hash_path):\n",
    "    \"\"\"загружаем хэши, вычисляем хэши, сохраняем хэши\"\"\"\n",
    "    try:\n",
    "        with codecs.open(hash_path,'r', encoding='utf-8') as file:\n",
    "            old_root = file.read()\n",
    "            old_root = json.loads(old_root)\n",
    "            print('readed',hash_path)\n",
    "            print(old_root.keys())\n",
    "    except BaseException as e:\n",
    "        print(e)\n",
    "        old_root = {}\n",
    "    return old_root\n",
    "\n",
    "def myjson_dumps(old_root):\n",
    "    return json.dumps(old_root,indent='\\t',ensure_ascii=False)\n",
    "\n",
    "def myjson_dump(old_root,hash_path):\n",
    "    try:\n",
    "        print('start writing')\n",
    "        with codecs.open(hash_path,'w', encoding='utf-8') as file:\n",
    "            s = myjson_dumps(old_root)\n",
    "            file.write(s)\n",
    "            print('writed',hash_path)\n",
    "    except BasicException as e:\n",
    "        print('start writing with exception',e)\n",
    "        with codecs.open(hash_path,'w', encoding='utf-8') as file:\n",
    "            s = myjson_dumps(old_root)\n",
    "            file.write(s)\n",
    "            print('writed',hash_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# diff, patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T06:37:07.545207Z",
     "start_time": "2019-11-13T06:37:07.515206Z"
    }
   },
   "outputs": [],
   "source": [
    "def path_diff(root,old_root):\n",
    "    new = {}\n",
    "    old = {}\n",
    "    strict_new = {}\n",
    "    strict_old = {}\n",
    "    modified = {}\n",
    "    def diff1(root,old_root,path):\n",
    "        nonlocal new\n",
    "        nonlocal old\n",
    "        nonlocal modified\n",
    "        for name in root.keys():\n",
    "            if type(root[name])==dict: # directory\n",
    "                if name in old_root and type(old_root[name])==dict:\n",
    "                    # same dirs\n",
    "                    diff1(root[name],old_root[name],path+[name])\n",
    "                elif name in old_root and type(old_root[name])==list:\n",
    "                    # file -> dir\n",
    "                    print('warning: file->dir :',my_path_join_a(*path,name))\n",
    "                    strict_old[tuple(path+[name])] = old_root[name]\n",
    "                    strict_new[tuple(path+[name])] =     root[name]\n",
    "                elif name in old_root:\n",
    "                    raise BaseException(path)\n",
    "                else:\n",
    "                    # new dir\n",
    "                    new[tuple(path+[name])] =     root[name]\n",
    "            elif type(root[name])==list: # file\n",
    "                assert len(root[name])==3\n",
    "                if name in old_root and type(old_root[name])==list:\n",
    "                    # file -> file\n",
    "                    assert len(old_root[name])==3\n",
    "                    if root[name][2]!=None and root[name][2]==old_root[name][2]:\n",
    "                        # same files\n",
    "                        pass\n",
    "                    elif root[name][2]==None and root[name][2]==old_root[name][2] and \\\n",
    "                      root[name][1]==old_root[name][1] and \\\n",
    "                      abs(root[name][0] - old_root[name][0]) < 1 : # 1 second between timestamps\n",
    "                        # same files without hashes\n",
    "                        pass\n",
    "                    else:\n",
    "                        # diff files\n",
    "                        modified[tuple(path+[name])] = \\\n",
    "                            (old_root[name], root[name])\n",
    "                elif name in old_root and type(old_root[name])==dict:\n",
    "                    # dir -> file\n",
    "                    print('warning: dir->file :',my_path_join_a(*path,name))\n",
    "                    strict_new[tuple(path+[name])] =     root[name]\n",
    "                    strict_old[tuple(path+[name])] = old_root[name]\n",
    "                elif name in old_root:\n",
    "                    raise BaseException(path)\n",
    "                else:\n",
    "                    # new file\n",
    "                    new[tuple(path+[name])] =     root[name]\n",
    "            else:\n",
    "                raise BaseException(path)\n",
    "        for name in old_root.keys():\n",
    "            if name not in root:\n",
    "                if type(old_root[name])==dict:\n",
    "                    # old dir\n",
    "                    old[tuple(path+[name])] = old_root[name]\n",
    "                elif type(old_root[name])==list:\n",
    "                    # old file\n",
    "                    old[tuple(path+[name])] = old_root[name]\n",
    "                else:\n",
    "                    raise BaseException(path)\n",
    "    diff1(root,old_root,[])\n",
    "    return (modified,old,new,strict_old,strict_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T06:37:08.056237Z",
     "start_time": "2019-11-13T06:37:08.045236Z"
    }
   },
   "outputs": [],
   "source": [
    "from copy import *\n",
    "\n",
    "def print_lines(arg,caption=None):\n",
    "    if caption!=None:\n",
    "        print(caption)\n",
    "    for x in arg:\n",
    "        print(x)\n",
    "        \n",
    "verbose = 0\n",
    "# 1 - стадии\n",
    "# 2 - изменения\n",
    "# 3 - пометки\n",
    "# 4 - более детально\n",
    "def vprint(v,*args):\n",
    "    if verbose>=v: print(*args)\n",
    "def dvr(fun):\n",
    "    def wrapper(prefix,path,subtree):\n",
    "        #vprint(1.5,'check',prefix,path)\n",
    "        tmp = fun(prefix,path,subtree)\n",
    "        if tmp!=None and tmp!=set() and 0:\n",
    "            print('after check',prefix,path)\n",
    "            print_lines(tmp)\n",
    "            print('---')\n",
    "        return tmp\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T06:37:08.572266Z",
     "start_time": "2019-11-13T06:37:08.562266Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_files(old):\n",
    "    old_files = {}\n",
    "    for p1,subtree in old.items():\n",
    "        for p2,v in tree_iterator(subtree):\n",
    "            if v[2] not in old_files:\n",
    "                old_files[v[2]] = set()\n",
    "            #print(p1,p2,tuple(p2))#,p1+tuple(p2))\n",
    "            old_files[v[2]].add(p1+tuple(p2))\n",
    "    return old_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T06:37:09.065294Z",
     "start_time": "2019-11-13T06:37:09.049294Z"
    }
   },
   "outputs": [],
   "source": [
    "CHANGED = 1\n",
    "CANDIDATES = 2\n",
    "def del_files_by_tree(files,prefix,subtree):\n",
    "    \"\"\"удаляет из files все файлы, перечисленные в subtree\n",
    "    prefix - путь к subtree\n",
    "    \"\"\"\n",
    "    if type(subtree)==list:\n",
    "        assert subtree[2] in files, subtree[2]\n",
    "        assert prefix in files[subtree[2]], (prefix,files[subtree[2]])\n",
    "        vprint(2,'del hash',subtree[2])\n",
    "        files[subtree[2]].remove(prefix)\n",
    "        if len(files[subtree[2]])==0:\n",
    "            del files[subtree[2]]\n",
    "    else:\n",
    "        for name in subtree.keys():\n",
    "            if type(name)==str:\n",
    "                del_files_by_tree(files,prefix+(name,),subtree[name])\n",
    "\n",
    "def check_variants(variants,new,names):\n",
    "    # variants - список путей, куда можно переместить данный объект\n",
    "    # names - \n",
    "    vars2 = set()\n",
    "    for p in variants: # по каждому варианту\n",
    "        for i in range(1,len(p)+1): # находим дерево в new, в котором находится данный путь\n",
    "            if p[:i] in new:\n",
    "                break\n",
    "        else: i=None # !!! такого вообще не должно быть\n",
    "        if i==None:\n",
    "            continue # не добавляем в vars2\n",
    "        new_tree = get_subtree(new[p[:i]],p[i:])\n",
    "        # !!! может оказаться файлом а не папкой, но не должно\n",
    "        # проверяем, все ли объекты внутри этой папки содержатся внутри текущей папки\n",
    "        for n1 in new_tree.keys():\n",
    "            if type(n1)==str and n1 not in names:\n",
    "                break # не добавляем в vars2\n",
    "        else:\n",
    "            vars2.add(p)\n",
    "    return vars2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T06:37:09.899342Z",
     "start_time": "2019-11-13T06:37:09.849339Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_moved(old_files,new_files,old,new):\n",
    "    moved = []\n",
    "    changed = True\n",
    "    #@dvr\n",
    "    def find_moved(forced,prefix_path,subtree):\n",
    "        \"\"\"возвращает список путей, куда можно переместить данный объект,\n",
    "        предварительно узнав, куда можно переместить все дочерние объекты\n",
    "        если данный объект никуда переместить нельзя\n",
    "            перемещает все вложенные объекты\n",
    "        forced - передается в move_obj\n",
    "        prefix_path - путь к данный объекту\n",
    "        subtree - данный объект\n",
    "        \"\"\"\n",
    "        if type(subtree)==list:\n",
    "            if subtree[2] in new_files:\n",
    "                return new_files[subtree[2]] # ? copy - не нужно, т.к. (1)\n",
    "            else: return set()\n",
    "        elif type(subtree)==dict:\n",
    "            # смотрим закэшированное\n",
    "            if CHANGED not in subtree and CANDIDATES in subtree:\n",
    "                vars2 = check_variants(subtree[CANDIDATES],new,subtree)\n",
    "                if len(vars2)>0:\n",
    "                    return vars2 # да, эту папку куда-то целиком переместить можно\n",
    "\n",
    "            # сначала проверяем, можно ли эту папку целиком куда-то переместить\n",
    "            variants = None if CHANGED in subtree else set()\n",
    "            names = {}\n",
    "            for name in subtree.keys():\n",
    "                if type(name)==str:\n",
    "                    v = find_moved(forced,prefix_path+(name,),subtree[name])\n",
    "                    if v == None or variants==None:\n",
    "                        variants = None\n",
    "                    else:\n",
    "                        v = {p[:-1] for p in v if p[-1]==name} #(1)\n",
    "                        variants &= v\n",
    "                    names[name] = v\n",
    "            if variants!=None and len(variants)>0:\n",
    "                # содержимое этой папки куда-то в одно место переместить можно\n",
    "                # но...\n",
    "                vars2 = check_variants(variants,new,names)\n",
    "                if len(vars2)>0:\n",
    "                    return vars2 # да, эту папку куда-то целиком переместить можно\n",
    "                \n",
    "            # если же целиком эту папку никуда переместить нельзя\n",
    "            # то начинаем определять, куда переместить ее содержимое\n",
    "            subtree[CHANGED] = True\n",
    "            fordel = set()\n",
    "            for name in subtree.keys():\n",
    "                if type(name)==str:\n",
    "                    variants = names[name]\n",
    "                    if move_obj(forced,variants,prefix_path+(name,),subtree[name]):\n",
    "                        fordel.add(name)\n",
    "            for name in fordel:\n",
    "                del subtree[name]\n",
    "            return None\n",
    "        else: raise BaseException(prefix+path)\n",
    "    def move_obj(forced,variants,path,subtree):\n",
    "        \"\"\"организует перемещение данного объекта, и возвращает, удалось ли это сделать\n",
    "        если forced==False - перемещает, только если есть единственный вариант\n",
    "        если forced==True - перемещает, если есть хотябы какой-то вариант\n",
    "        variants - варианты перемещения\n",
    "        path - путь к данному объекту\n",
    "        subtree - данный объект\n",
    "        \"\"\"\n",
    "        nonlocal changed\n",
    "        #nonlocal moved\n",
    "        if variants!=None and len(variants)>1 and not forced:\n",
    "            # несколько вариантов - кэшируем их в CANDIDATES\n",
    "            if type(subtree)==dict:\n",
    "                subtree[CANDIDATES] = variants\n",
    "            return False\n",
    "        elif variants!=None and (len(variants)==1 or len(variants)>0 and forced):\n",
    "            # перемещаем\n",
    "            changed = True\n",
    "            dest_p = next(iter(variants))\n",
    "            moved.append((path,dest_p,deepcopy(subtree)))\n",
    "            # удаляем из old и old_files\n",
    "            del_files_by_tree(old_files,path,subtree)\n",
    "            #fordel.add(name) -> return True\n",
    "            # удаляем из new и new_files\n",
    "            for i in range(1,len(dest_p)+1):\n",
    "                if dest_p[:i] in new:\n",
    "                    break\n",
    "            else: raise BaseException(path)\n",
    "            if len(dest_p)==i:\n",
    "                del_files_by_tree(new_files,dest_p,new[dest_p])\n",
    "                del new[dest_p]\n",
    "            else:\n",
    "                dest_parent = get_subtree(new[dest_p[:i]],dest_p[i:-1])\n",
    "                del_files_by_tree(new_files,dest_p,dest_parent[dest_p[-1]])\n",
    "                del dest_parent[dest_p[-1]]\n",
    "                dest_parent[CHANGED]=True\n",
    "            return True\n",
    "        else:\n",
    "            # оставляем как есть\n",
    "            return False\n",
    "        \n",
    "    # перемещаем, если это можно сделать единственным образом\n",
    "    while changed:\n",
    "        print('--- ITERATION ---')\n",
    "        changed = False\n",
    "        fordel = set()\n",
    "        for p1,subtree in old.items():\n",
    "            variants = find_moved(False,p1,subtree)\n",
    "            if move_obj(False,variants,p1,old[p1]):\n",
    "                fordel.add(p1)\n",
    "        for p1 in fordel:\n",
    "            del old[p1]\n",
    "    \n",
    "    # перемещаем куда попало\n",
    "    if 1:\n",
    "        print('--- FINAL ITERATION ---')\n",
    "        fordel = set()\n",
    "        for p1,subtree in old.items():\n",
    "            variants = find_moved(True,p1,subtree)\n",
    "            if move_obj(True,variants,p1,old[p1]):\n",
    "                fordel.add(p1)\n",
    "        for p1 in fordel:\n",
    "            del old[p1]\n",
    "    return moved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T06:37:10.668386Z",
     "start_time": "2019-11-13T06:37:10.638384Z"
    }
   },
   "outputs": [],
   "source": [
    "def hash_diff(old,new):\n",
    "    # копирую old, new\n",
    "    # создаю old_files, new_files\n",
    "    # создаю moved, обходя и меняя old и new (несколько раз) (а также old_files и new_files)\n",
    "    # проверяю пересечение old_files и new_files\n",
    "    # раскладываю old, создаю old1, проверяю old_files\n",
    "    # раскладываю new, создаю new1, проверяю new_files\n",
    "    # возвращаю moved, old, new, old1, new1\n",
    "    #     old, new - содержат папки для создания и удаления перед и после перемещением\n",
    "\n",
    "    # копирую old, new\n",
    "    old = deepcopy(old)\n",
    "    new = deepcopy(new)\n",
    "    \n",
    "    # создаю old_files, new_files\n",
    "    old_files = make_files(old)\n",
    "    new_files = make_files(new)\n",
    "            \n",
    "    #print_lines(old.keys(),'--- old.keys ---')\n",
    "    #print_lines(new.keys(),'--- new.keys ---')\n",
    "    #print_lines(old.items(),'--- old.items ---')\n",
    "    #print_lines(new.items(),'--- new.items ---')\n",
    "    #print_lines(old_files.items(),'--- old_files ---')\n",
    "    #print_lines(new_files.items(),'--- new_files ---')\n",
    "    \n",
    "    # создаю moved, обходя и меняя old и new (несколько раз) (а также old_files и new_files)\n",
    "    moved = make_moved(old_files,new_files,old,new)\n",
    "    \n",
    "    #print_lines(moved,'--- moved ---')\n",
    "    #print_lines(old.keys(),'--- old.keys ---')\n",
    "    #print_lines(new.keys(),'--- new.keys ---')\n",
    "    #print_lines(old_files.items(),'--- old_files ---')\n",
    "    #print_lines(new_files.items(),'--- new_files ---')\n",
    "\n",
    "    # проверяю пересечение old_files и new_files\n",
    "    #print('--- checking ---')\n",
    "    flag = False\n",
    "    for h1 in old_files:\n",
    "        if h1 in new_files:\n",
    "            flag = True\n",
    "            #print(h1)\n",
    "    if flag:\n",
    "        raise BaseException(h1)\n",
    "            \n",
    "    # раскладываю old, создаю old1, проверяю old_files\n",
    "        # целиковые папки остануться целиковыми, их не нужно снова конструировать\n",
    "    def clear_dir(files,prefix,path,subtree):\n",
    "        \"\"\"вызывается от директории\n",
    "        очищает директорию от CANDIDATES\n",
    "        файлы удаляет из files\n",
    "        \"\"\"\n",
    "        if CANDIDATES in subtree: del subtree[CANDIDATES]\n",
    "        for name in subtree:\n",
    "            if type(subtree[name])==list:\n",
    "                assert subtree[name][2] in files, (subtree[name][2],prefix,path)\n",
    "                files[subtree[name][2]].remove(prefix+path+(name,))\n",
    "                if len(files[subtree[name][2]])==0:\n",
    "                    del files[subtree[name][2]]\n",
    "            else:\n",
    "                clear_dir(files,prefix,path+(name,),subtree[name])\n",
    "        \n",
    "    def q_need_move(files,old1,prefix,path,subtree):\n",
    "        \"\"\"вызывается от директории или файла\n",
    "        если это файл или целикова директория - возвращает True и перемещает в old1\n",
    "        очищает директоию от CHANGED, \n",
    "        файлы удаляет из files\n",
    "        директории очищает\n",
    "        \"\"\"\n",
    "        if type(subtree)==list:\n",
    "            assert subtree[2] in files, (subtree[2],prefix,path)\n",
    "            files[subtree[2]].remove(prefix+path)\n",
    "            if len(files[subtree[2]])==0:\n",
    "                del files[subtree[2]]\n",
    "            return True\n",
    "        elif CHANGED not in subtree:\n",
    "            clear_dir(files,prefix,path,subtree)\n",
    "            return True\n",
    "        else:\n",
    "            del subtree[CHANGED]\n",
    "            fordel = set()\n",
    "            for name in subtree:\n",
    "                if q_need_move(files,old1,prefix,path+(name,),subtree[name]):\n",
    "                    old1[prefix+path+(name,)] = subtree[name]\n",
    "                    fordel.add(name)\n",
    "            for name in fordel:\n",
    "                del subtree[name]\n",
    "            return False\n",
    "            \n",
    "    old1 = {}\n",
    "    fordel = set()\n",
    "    for p1,subdir in old.items():\n",
    "        if q_need_move(old_files,old1,p1,(),subdir):\n",
    "            old1[p1] = subdir\n",
    "            fordel.add(p1)\n",
    "    for name in fordel:\n",
    "        del old[name]\n",
    "    assert len(old_files)==0\n",
    "        \n",
    "    # раскладываю new, создаю new1, проверяю new_files\n",
    "    new1 = {}\n",
    "    fordel = set()\n",
    "    for p1,subdir in new.items():\n",
    "        if q_need_move(new_files,new1,p1,(),subdir):\n",
    "            new1[p1] = subdir\n",
    "            fordel.add(p1)\n",
    "    for name in fordel:\n",
    "        del new[name]\n",
    "    assert len(new_files)==0\n",
    "        \n",
    "    # возвращаю moved, old1, new1\n",
    "    return (moved,old,new,old1,new1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T06:37:11.334424Z",
     "start_time": "2019-11-13T06:37:11.331424Z"
    }
   },
   "outputs": [],
   "source": [
    "# path_patch, path_patch_back\n",
    "# hash_patch, hash_patch_back\n",
    "# path_sync\n",
    "# hash_sync\n",
    "# tree_dump, tree_load\n",
    "# path_patch_dump, path_patch_load\n",
    "# hash_patch_dump, hash_patch_load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T06:37:12.447488Z",
     "start_time": "2019-11-13T06:37:12.433487Z"
    }
   },
   "outputs": [],
   "source": [
    "def tree_dump(root):\n",
    "    if type(root)==list:\n",
    "        return str(root[0])+' '+str(root[1])+' '+str(root[2])\n",
    "        # root[2] обычно строка, но иногда это None\n",
    "    else:\n",
    "        new_root = {}\n",
    "        for name in root.keys():\n",
    "            assert type(name)==str\n",
    "            tmp_root = root\n",
    "            name_path = name\n",
    "            while type(tmp_root[name])==dict and len(tmp_root[name])==1:\n",
    "                subname = next(iter(tmp_root[name]))\n",
    "                assert type(subname)==str\n",
    "                tmp_root = tmp_root[name]\n",
    "                name_path+='/'+subname\n",
    "                name = subname\n",
    "            new_root[name_path] = tree_dump(tmp_root[name])\n",
    "        return new_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T06:37:13.039522Z",
     "start_time": "2019-11-13T06:37:13.028521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x/y': '1 2 3'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_dump({\n",
    "    'x':{\n",
    "        'y':[1,2,'3'],\n",
    "        #'z':[1,2,'3'],\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T06:37:14.512606Z",
     "start_time": "2019-11-13T06:37:14.500605Z"
    }
   },
   "outputs": [],
   "source": [
    "def path_patch_dump(in_modified,in_old,in_new,in_strict_old,in_strict_new):\n",
    "    modified = {}\n",
    "    for path,v in in_modified.items():\n",
    "        modified['/'.join(path)] = (tree_dump(v[0]),tree_dump(v[1]))\n",
    "    old = {}\n",
    "    for path,v in in_old.items():\n",
    "        old['/'.join(path)] = tree_dump(v)\n",
    "    new = {}\n",
    "    for path,v in in_new.items():\n",
    "        new['/'.join(path)] = tree_dump(v)\n",
    "    strict_old = {}\n",
    "    for path,v in in_strict_old.items():\n",
    "        strict_old['/'.join(path)] = tree_dump(v)\n",
    "    strict_old = {}\n",
    "    for path,v in in_strict_new.items():\n",
    "        strict_new['/'.join(path)] = tree_dump(v)\n",
    "    return {\n",
    "        'modified':modified,\n",
    "        'old':old,\n",
    "        'new':new,\n",
    "        'strict_old':strict_old,\n",
    "        'strict_new':strict_new\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T07:46:24.557033Z",
     "start_time": "2019-11-13T07:46:24.541032Z"
    }
   },
   "outputs": [],
   "source": [
    "# перед применением рекомендуется:\n",
    "#   проверить, что old и strict_old не пересекаются (по ключам) и объединить их\n",
    "#   проверить, что new и strict_new не пересекаются (по ключам) и объединить их\n",
    "def path_patch(old_root,modified,old,new):\n",
    "    # по всем modified\n",
    "    #   проверяем, что имеется по старому пути\n",
    "    #   и заменяем на новое значение (через parent)\n",
    "    # по всем old\n",
    "    #   проверяем, что имеется по старому пути\n",
    "    #   и удаляем это (через parent)\n",
    "    # по всем new\n",
    "    #   создаем это (через parent)\n",
    "    root = deepcopy(old_root)\n",
    "    for path,(old_file,new_file) in modified.items():\n",
    "        parent = get_subtree(root,path[:-1])\n",
    "        assert parent[path[-1]] == old_file\n",
    "        parent[path[-1]] = deepcopy(new_file)\n",
    "    for path,obj in old.items():\n",
    "        parent = get_subtree(root,path[:-1])\n",
    "        assert parent[path[-1]] == obj\n",
    "        del parent[path[-1]]\n",
    "    for path,obj in new.items():\n",
    "        parent = get_subtree(root,path[:-1])\n",
    "        assert path[-1] not in parent\n",
    "        parent[path[-1]] = deepcopy(obj)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T07:46:19.000538Z",
     "start_time": "2019-11-13T07:46:18.980537Z"
    }
   },
   "outputs": [],
   "source": [
    "def hash_patch(old_root,modified,moved,old_dirs,new_dirs,old,new):\n",
    "    # всё модифицируем\n",
    "    # всё удаляем\n",
    "    # создаём все новые папки\n",
    "    # всё перемещаем\n",
    "    # удаляем все старые папки\n",
    "    # всё создаем\n",
    "    root = deepcopy(old_root)\n",
    "    # всё модифицируем\n",
    "    for path,(old_file,new_file) in modified.items():\n",
    "        parent = get_subtree(root,path[:-1])\n",
    "        assert parent[path[-1]] == old_file\n",
    "        parent[path[-1]] = deepcopy(new_file)\n",
    "    # всё удаляем\n",
    "    for path,obj in old.items():\n",
    "        parent = get_subtree(root,path[:-1])\n",
    "        assert parent[path[-1]] == obj\n",
    "        del parent[path[-1]]\n",
    "    # создаём все новые папки\n",
    "    for path,obj in new_dirs.items():\n",
    "        parent = get_subtree(root,path[:-1])\n",
    "        assert path[-1] not in parent\n",
    "        parent[path[-1]] = deepcopy(obj)\n",
    "    # всё перемещаем\n",
    "    for from_p,to_p,obj in moved:\n",
    "        parent = get_subtree(root,from_p[:-1])\n",
    "        assert parent[from_p[-1]] == obj\n",
    "        del parent[from_p[-1]]\n",
    "        \n",
    "        parent = get_subtree(root,to_p[:-1])\n",
    "        assert to_p[-1] not in parent\n",
    "        parent[to_p[-1]] = deepcopy(obj)\n",
    "    # удаляем все старые папки\n",
    "    for path,obj in old_dirs.items():\n",
    "        parent = get_subtree(root,path[:-1])\n",
    "        assert parent[path[-1]] == obj\n",
    "        del parent[path[-1]]\n",
    "    # всё создаем\n",
    "    for path,obj in new.items():\n",
    "        parent = get_subtree(root,path[:-1])\n",
    "        assert path[-1] not in parent\n",
    "        parent[path[-1]] = deepcopy(obj)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T07:05:00.435683Z",
     "start_time": "2019-11-13T07:05:00.423682Z"
    }
   },
   "outputs": [],
   "source": [
    "def first_diff(old_root,root):\n",
    "    \"\"\"сравнивает 2 дерева файлов\n",
    "    если они равны - возвращает True\n",
    "    иначе находит первое различие и возвращает путь и пару элементов\n",
    "    \"\"\"\n",
    "    #if root==old_root:                   return True\n",
    "    if type(root)!=type(old_root):       return ((),(old_root,root))\n",
    "    if type(root)==list:\n",
    "        if root[1]==old_root[1] and root[2]==old_root[2]:\n",
    "                                         return True\n",
    "        else:                            return ((),(old_root,root))\n",
    "    if type(root)==dict:\n",
    "        if root.keys()!=old_root.keys(): return ((),(old_root.keys()-root.keys(),\n",
    "                                                     root.keys()-old_root.keys()))\n",
    "        for k in root:\n",
    "            d = first_diff(old_root[k],root[k])\n",
    "            if d!=True:                  return ((k,*d[0]),d[1])\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# применение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T06:42:22.521136Z",
     "start_time": "2019-11-13T06:37:39.644612Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readed D:\\.files\\111.json\n",
      "dict_keys(['$RECYCLE.BIN', 'amd64', 'Config.Msi', 'Games', 'i386', 'msdownld.tmp', 'System Volume Information', 'treeinfo.wc', 'Users', 'Windows.old'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14e311bea744c1dae4b2431c200b64f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Config.Msi\n",
      "[WinError 5] Отказано в доступе: 'D:\\\\Config.Msi'\n",
      "\n",
      "D:\\System Volume Information\n",
      "[WinError 5] Отказано в доступе: 'D:\\\\System Volume Information'\n",
      "\n",
      "D:\\Users\\feelus\\AppData\\Local\\ElevatedDiagnostics\n",
      "[WinError 5] Отказано в доступе: 'D:\\\\Users\\\\feelus\\\\AppData\\\\Local\\\\ElevatedDiagnostics'\n",
      "\n",
      "D:\\Users\\feelus\\AppData\\Local\\Temp\\msdtadmin\n",
      "[WinError 5] Отказано в доступе: 'D:\\\\Users\\\\feelus\\\\AppData\\\\Local\\\\Temp\\\\msdtadmin'\n",
      "\n",
      "D:\\Users\\feelus\\Local Settings\\ElevatedDiagnostics\n",
      "[WinError 5] Отказано в доступе: 'D:\\\\Users\\\\feelus\\\\Local Settings\\\\ElevatedDiagnostics'\n",
      "\n",
      "D:\\Users\\feelus\\Local Settings\\Temp\\msdtadmin\n",
      "[WinError 5] Отказано в доступе: 'D:\\\\Users\\\\feelus\\\\Local Settings\\\\Temp\\\\msdtadmin'\n",
      "\n",
      "D:\\Users\\test\\AppData\\Local\n",
      "[WinError 5] Отказано в доступе: 'D:\\\\Users\\\\test\\\\AppData\\\\Local'\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26f0a1eedbc466eac534a34193a886b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Users\\feelus\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\Application Cache\\Cache\\data_0\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\\\\Default\\\\Application Cache\\\\Cache\\\\data_0'\n",
      "\n",
      "D:\\Users\\feelus\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\Application Cache\\Cache\\data_1\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\\\\Default\\\\Application Cache\\\\Cache\\\\data_1'\n",
      "\n",
      "D:\\Users\\feelus\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\Application Cache\\Cache\\data_2\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\\\\Default\\\\Application Cache\\\\Cache\\\\data_2'\n",
      "\n",
      "D:\\Users\\feelus\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\Application Cache\\Cache\\data_3\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\\\\Default\\\\Application Cache\\\\Cache\\\\data_3'\n",
      "\n",
      "D:\\Users\\feelus\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\Cache\\data_0\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\\\\Default\\\\Cache\\\\data_0'\n",
      "\n",
      "D:\\Users\\feelus\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\Cache\\data_1\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\\\\Default\\\\Cache\\\\data_1'\n",
      "\n",
      "D:\\Users\\feelus\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\Cache\\data_2\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\\\\Default\\\\Cache\\\\data_2'\n",
      "\n",
      "D:\\Users\\feelus\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\Cache\\data_3\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\\\\Default\\\\Cache\\\\data_3'\n",
      "\n",
      "D:\\Users\\feelus\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\Current Session\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\\\\Default\\\\Current Session'\n",
      "\n",
      "D:\\Users\\feelus\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\Current Tabs\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\\\\Default\\\\Current Tabs'\n",
      "\n",
      "D:\\Users\\feelus\\AppData\\Local\\Google\\Chrome\\User Data\\lockfile\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\\\\lockfile'\n",
      "\n",
      "D:\\Users\\feelus\\AppData\\Local\\Google\\Chrome\\User Data\\ShaderCache\\GPUCache\\data_0\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\\\\ShaderCache\\\\GPUCache\\\\data_0'\n",
      "\n",
      "D:\\Users\\feelus\\AppData\\Local\\Google\\Chrome\\User Data\\ShaderCache\\GPUCache\\data_1\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\\\\ShaderCache\\\\GPUCache\\\\data_1'\n",
      "\n",
      "D:\\Users\\feelus\\AppData\\Local\\Google\\Chrome\\User Data\\ShaderCache\\GPUCache\\data_3\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\\\\ShaderCache\\\\GPUCache\\\\data_3'\n",
      "\n",
      "D:\\Users\\feelus\\AppData\\Local\\Microsoft\\Windows\\UsrClass.dat\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\AppData\\\\Local\\\\Microsoft\\\\Windows\\\\UsrClass.dat'\n",
      "\n",
      "D:\\Users\\feelus\\AppData\\Local\\Microsoft\\Windows\\UsrClass.dat.LOG1\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\AppData\\\\Local\\\\Microsoft\\\\Windows\\\\UsrClass.dat.LOG1'\n",
      "\n",
      "D:\\Users\\feelus\\AppData\\Local\\Microsoft\\Windows\\WebCache\\V01.log\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\AppData\\\\Local\\\\Microsoft\\\\Windows\\\\WebCache\\\\V01.log'\n",
      "\n",
      "D:\\Users\\feelus\\AppData\\Local\\Microsoft\\Windows\\WebCache\\WebCacheV01.dat\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\AppData\\\\Local\\\\Microsoft\\\\Windows\\\\WebCache\\\\WebCacheV01.dat'\n",
      "\n",
      "D:\\Users\\feelus\\AppData\\Local\\Microsoft\\Windows\\WebCache\\WebCacheV01.tmp\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\AppData\\\\Local\\\\Microsoft\\\\Windows\\\\WebCache\\\\WebCacheV01.tmp'\n",
      "\n",
      "D:\\Users\\feelus\\Local Settings\\Google\\Chrome\\User Data\\Default\\Application Cache\\Cache\\data_0\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\Local Settings\\\\Google\\\\Chrome\\\\User Data\\\\Default\\\\Application Cache\\\\Cache\\\\data_0'\n",
      "\n",
      "D:\\Users\\feelus\\Local Settings\\Google\\Chrome\\User Data\\Default\\Application Cache\\Cache\\data_1\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\Local Settings\\\\Google\\\\Chrome\\\\User Data\\\\Default\\\\Application Cache\\\\Cache\\\\data_1'\n",
      "\n",
      "D:\\Users\\feelus\\Local Settings\\Google\\Chrome\\User Data\\Default\\Application Cache\\Cache\\data_2\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\Local Settings\\\\Google\\\\Chrome\\\\User Data\\\\Default\\\\Application Cache\\\\Cache\\\\data_2'\n",
      "\n",
      "D:\\Users\\feelus\\Local Settings\\Google\\Chrome\\User Data\\Default\\Application Cache\\Cache\\data_3\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\Local Settings\\\\Google\\\\Chrome\\\\User Data\\\\Default\\\\Application Cache\\\\Cache\\\\data_3'\n",
      "\n",
      "D:\\Users\\feelus\\Local Settings\\Google\\Chrome\\User Data\\Default\\Cache\\data_0\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\Local Settings\\\\Google\\\\Chrome\\\\User Data\\\\Default\\\\Cache\\\\data_0'\n",
      "\n",
      "D:\\Users\\feelus\\Local Settings\\Google\\Chrome\\User Data\\Default\\Cache\\data_1\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\Local Settings\\\\Google\\\\Chrome\\\\User Data\\\\Default\\\\Cache\\\\data_1'\n",
      "\n",
      "D:\\Users\\feelus\\Local Settings\\Google\\Chrome\\User Data\\Default\\Cache\\data_2\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\Local Settings\\\\Google\\\\Chrome\\\\User Data\\\\Default\\\\Cache\\\\data_2'\n",
      "\n",
      "D:\\Users\\feelus\\Local Settings\\Google\\Chrome\\User Data\\Default\\Cache\\data_3\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\Local Settings\\\\Google\\\\Chrome\\\\User Data\\\\Default\\\\Cache\\\\data_3'\n",
      "\n",
      "D:\\Users\\feelus\\Local Settings\\Google\\Chrome\\User Data\\Default\\Current Session\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\Local Settings\\\\Google\\\\Chrome\\\\User Data\\\\Default\\\\Current Session'\n",
      "\n",
      "D:\\Users\\feelus\\Local Settings\\Google\\Chrome\\User Data\\Default\\Current Tabs\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\Local Settings\\\\Google\\\\Chrome\\\\User Data\\\\Default\\\\Current Tabs'\n",
      "\n",
      "D:\\Users\\feelus\\Local Settings\\Google\\Chrome\\User Data\\lockfile\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\Local Settings\\\\Google\\\\Chrome\\\\User Data\\\\lockfile'\n",
      "\n",
      "D:\\Users\\feelus\\Local Settings\\Google\\Chrome\\User Data\\ShaderCache\\GPUCache\\data_0\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\Local Settings\\\\Google\\\\Chrome\\\\User Data\\\\ShaderCache\\\\GPUCache\\\\data_0'\n",
      "\n",
      "D:\\Users\\feelus\\Local Settings\\Google\\Chrome\\User Data\\ShaderCache\\GPUCache\\data_1\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\Local Settings\\\\Google\\\\Chrome\\\\User Data\\\\ShaderCache\\\\GPUCache\\\\data_1'\n",
      "\n",
      "D:\\Users\\feelus\\Local Settings\\Google\\Chrome\\User Data\\ShaderCache\\GPUCache\\data_3\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\Local Settings\\\\Google\\\\Chrome\\\\User Data\\\\ShaderCache\\\\GPUCache\\\\data_3'\n",
      "\n",
      "D:\\Users\\feelus\\Local Settings\\Microsoft\\Windows\\UsrClass.dat\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\Local Settings\\\\Microsoft\\\\Windows\\\\UsrClass.dat'\n",
      "\n",
      "D:\\Users\\feelus\\Local Settings\\Microsoft\\Windows\\UsrClass.dat.LOG1\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\Local Settings\\\\Microsoft\\\\Windows\\\\UsrClass.dat.LOG1'\n",
      "\n",
      "D:\\Users\\feelus\\Local Settings\\Microsoft\\Windows\\WebCache\\V01.log\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\Local Settings\\\\Microsoft\\\\Windows\\\\WebCache\\\\V01.log'\n",
      "\n",
      "D:\\Users\\feelus\\Local Settings\\Microsoft\\Windows\\WebCache\\WebCacheV01.dat\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\Local Settings\\\\Microsoft\\\\Windows\\\\WebCache\\\\WebCacheV01.dat'\n",
      "\n",
      "D:\\Users\\feelus\\Local Settings\\Microsoft\\Windows\\WebCache\\WebCacheV01.tmp\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\Local Settings\\\\Microsoft\\\\Windows\\\\WebCache\\\\WebCacheV01.tmp'\n",
      "\n",
      "D:\\Users\\feelus\\NTUSER.DAT\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\NTUSER.DAT'\n",
      "\n",
      "D:\\Users\\feelus\\ntuser.dat.LOG1\n",
      "[Errno 13] Permission denied: 'D:\\\\Users\\\\feelus\\\\ntuser.dat.LOG1'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_root = myjson_load(r'D:\\.files\\111.json')\n",
    "#old_root = deepcopy(root)\n",
    "root = scan('D:\\\\',{r'D:\\Users\\feelus\\YandexDisk',r'D:\\$RECYCLE.BIN'})\n",
    "calc_hashes(root,old_root,'D:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T06:43:24.012433Z",
     "start_time": "2019-11-13T06:43:22.684357Z"
    }
   },
   "outputs": [],
   "source": [
    "modified,old,new,strict_old,strict_new = path_diff(root,old_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T07:12:49.628682Z",
     "start_time": "2019-11-13T07:12:41.480216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, {}, {}, {}, {})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert set(old)&set(strict_old) == set()\n",
    "assert set(new)&set(strict_new) == set()\n",
    "new_root = path_patch(old_root,modified,{**old,**strict_old},{**new,**strict_new})\n",
    "path_diff(root,new_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T07:05:05.578977Z",
     "start_time": "2019-11-13T07:05:05.570977Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((), (set(), {'.files'}))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_diff(old_root,root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T07:08:24.513342Z",
     "start_time": "2019-11-13T07:08:24.510342Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(myjson_dumps(path_patch_dump(*path_diff(\n",
    "#    path_patch(old_root,modified,{**old,**strict_old},{**new,**strict_new}),\n",
    "#              old_root))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T07:08:27.006485Z",
     "start_time": "2019-11-13T07:08:27.004485Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(myjson_dumps(path_patch_dump(modified,old,new,strict_old,strict_new)))\n",
    "#myjson_dump(tree_dump(old_root),'old_root.json')\n",
    "#json.dumps(tree_dump(old_root['amd64']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T07:33:39.837202Z",
     "start_time": "2019-11-13T07:33:31.159068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ITERATION ---\n",
      "--- ITERATION ---\n",
      "--- FINAL ITERATION ---\n"
     ]
    }
   ],
   "source": [
    "modified,old,new,strict_old,strict_new = path_diff(root,old_root)\n",
    "assert set(old)&set(strict_old) == set()\n",
    "assert set(new)&set(strict_new) == set()\n",
    "moved,old_dirs,new_dirs,old,new = hash_diff(old_d,new_d)\n",
    "assert set(old)&set(strict_old) == set()\n",
    "assert set(new)&set(strict_new) == set()\n",
    "new_root = hash_patch(old_root,modified,moved,old_dirs,new_dirs,\n",
    "                      {**old,**strict_old},{**new,**strict_new})\n",
    "path_diff(root,new_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T07:34:02.227736Z",
     "start_time": "2019-11-13T07:34:01.293920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_diff(root,new_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T07:08:39.036173Z",
     "start_time": "2019-11-13T07:08:38.948168Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1039\n",
      "874\n",
      "--- ITERATION ---\n",
      "--- ITERATION ---\n",
      "--- FINAL ITERATION ---\n"
     ]
    }
   ],
   "source": [
    "len(old)\n",
    "old_d = {}#deepcopy(old)\n",
    "keys = [k for k in old.keys()]\n",
    "for k in keys:#[510:520]:\n",
    "    old_d[k]=deepcopy(old[k])\n",
    "print(len(old_d))\n",
    "\n",
    "len(new)\n",
    "new_d = {}#deepcopy(old)\n",
    "keys = [k for k in new.keys()]\n",
    "for k in keys:#[510:520]:\n",
    "    new_d[k]=deepcopy(new[k])\n",
    "print(len(new_d))\n",
    "\n",
    "\n",
    "r = hash_diff(old_d,new_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T07:59:03.394569Z",
     "start_time": "2019-11-12T07:55:38.354Z"
    }
   },
   "outputs": [],
   "source": [
    "x = {1:2,3:4}\n",
    "for y in x:\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T07:59:03.396569Z",
     "start_time": "2019-11-12T07:55:38.359Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    old = {\n",
    "        ('a','b'):[1,1,'1']\n",
    "    }\n",
    "    new = {\n",
    "        ('b','c'):[2,2,'2']\n",
    "    }\n",
    "if 0:\n",
    "    old = {\n",
    "        ('a','b'):[1,1,'1']\n",
    "    }\n",
    "    new = {\n",
    "        ('b','c'):[2,2,'1']\n",
    "    }\n",
    "if 0:\n",
    "    old = {\n",
    "        ('a','b'):[1,1,'1'],\n",
    "        ('a','c'):[1,1,'1'],\n",
    "    }\n",
    "    new = {\n",
    "        ('b','c'):[2,2,'1']\n",
    "    }\n",
    "if 1:\n",
    "    old = {\n",
    "        ('a','b'):[1,1,'1'],\n",
    "        ('a','c'):[1,1,'1'],\n",
    "    }\n",
    "    new = {\n",
    "        ('b','c'):[2,2,'1'],\n",
    "        ('b','d'):[2,2,'1'],\n",
    "    }\n",
    "r = hash_diff(old,new)\n",
    "\n",
    "print('moved',r[0])\n",
    "print('old',r[1])\n",
    "print('new',r[2])\n",
    "print('old1',r[3])\n",
    "print('new1',r[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

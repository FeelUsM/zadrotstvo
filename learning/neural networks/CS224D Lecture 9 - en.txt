1 (1) Lecture 9: (Wrap up: LSTMs and) Recursive Neural Networks
	1
	00:00:00,030 --> 00:00:03,480

	ninth lecture of CS 224 d 
	today we'll talk about recursive neural networks 
	I have a somewhat personally relationship with these since I've worked a lot of these kinds of models during my PhD 

2(2) Overview
	6
	00:00:11,610 --> 00:00:14,160

	so we'll actually spend two lectures on these 
	just like recurrent neural networks are pretty important 
	they're very flexible kinds of models and so basically today will motivate them a little bit with so called compositionality meaning 
	you know a sort of technique to get to the meaning of longer phrases 
	and then we'll actually look at a slightly different kind of objective function a structure prediction kind of objective function today and back propagation through structure is really going to be very similar to back propagation which you already know with just three you know slight modifications
	and then the nice thing is that these models are very general 
	and so if we have enough time at the end we can also actually have you know two or three slides on a computer vision example turns out the exact same model will work for images as well 
	and then in the next lecture we can go and look into a couple of modifications couple of extensions to the standard recursive neural networks 
	when I mention RNN today it will be mostly recursive neural networks it's unfortunate because our nents 
	and the last couple of lectures were a recurrent neural networks 
	and we'll actually look into the difference between those two as well

3 (11) Building on Word Vector Space Models
	39
	00:01:21,540 --> 00:01:24,990

	all right so we're very familiar by now with word vector space models 
	where you know words that are similar in meaning and have similar kinds of part of speech tags are close by so all the nouns are clustered together and inside the noun cluster would have you know different semantic sub structures 
	so the vector of germany would be closer to the vector of France for instance and Monday and Tuesday will be closer to one another 
	but of course words never appear in isolation and so so far we haven't really thought about what happens to vector representations when we want to represent specific short sub phrases 
	so what if I have the two phrases 'the country of my birth' and 'the place where I was born' for instance 
	if I was to build a text summarization system clearly I wouldn't want to have both of these together despite you know country AND place not being that similar and birth as a you know noun and 'where I was born' as a verb 
	but ideally if we build a text summarization(?) system we wouldn't include both of these phrases right 
	and so the question is how should we represent the meaning of these longer phrases 
	and the answer that is similar you know to or the answer will basically give to that today is by mapping those phrases into the exact same vector space where we had words 

	despite - несмотря на

4 (12) Semantic Vector Spaces
	73
	00:02:46,069 --> 00:02:47,900

	and so just to give you a sort of broader sense of where that puts us 
	there are a lot of these representations for single word vectors 
	we look through word2vek and and Club vectors basically they're sort of distributed and distributional techniques 
	it's actually a lot of other ones such as brown clusters which we're not going to cover much in this class 
	but basically they also capture core current statistics and so they are great but they really can't(?) capture longer phrases 
	and then there are a lot of other models that are very widely used such as bag of words models or the PCA based models 
	that we discovered for document representations and those are reasonably good for information retrieval 
	we just want to find specific documents and mention specific phrases or document exploration 
	we just want to know these documents roughly about politics or sports or things like that 
	but of course they ignore the word order 
	and so we can't really get too detailed understanding inside our representation 
	and so today we'll look at another technique for finding vectors that really represent phrases and sentences without ignoring word order and actually trying to capture both the syntactic structure as well as the semantic information of those phrases 

5 (13) How should we map phrases into a vector space?
	106
	00:04:01,219 --> 00:04:03,109

	so that leads us to the question of how we should go about doing this 
	and in this lecture we'll introduce and use the principle of compositionality which in our case states that the meaning vector of a sentence is determined by one the meaning of its words the word vectors 
	and throughout the lecture I will describe those I straight those in two dimensions but of course there will generally be 21 to 300 dimensional so much larger dimensional 
	and to the rules that are used to combine them 
	and unlike before where we just go from left to right and we just compute one vector for everything that we've read so far from the left side 
	this time we'll actually adhere to the grammatical structure of the sentence 
	and we'll try to find specific noun phrases that go together 
	so for instance "my birth" is by itself a reasonably grammatical or syntactic phrase and similarly the country is a noun phrase 
	and so we can combine those two 
	and then of my birth is actually a prepositional phrase 
	so those the word vector of will be combined with the word vector of my birth and so on 
	and then ideally at the end of it the final resulting vector will actually be somewhere close to other countries in a single word vector space 
	and so the neat thing about these models today is that they basically can jointly learn these so-called parse trees and compositional vector representations that ideally captures syntactic and semantic information of the phrases underneath them and not of the entire context that we've had so far to the left 

6 (14) Sentence Parsing: What we want
	148
	00:05:44,479 --> 00:05:46,550

	so what is what is sentence parsing 
	what when we say we want to parse a sentence what do we actually want out of it 
	so let's assume we start with a sentence like "the cat sat on the mat" very simple phrase 
	what were sentence what we would want is to have a model that understands that the cat is a proper noun phrase 
	and I could now replace that noun phrase with any other noun phrase and it would still be a grammatical English sentence 
	so I could say "the car sat on the mat" or "the whole class sat on the mat" 
	like any kind of noun phrase could now be replaced with this one and it would still be a grammatical sentence 
	similarly "the mat" is a noun phrase and now "on the mat" is a prepositional phrase 
	and again we could replace that prepositional phrase with any other one and still get a syntactically or grammatically plausible English sentence 
	so we could say "the cat sat" beyond the scope of this lecture that doesn't make any semantic sense but is actually a grammatically correct sentence 
	and that is that is a big an important distinction right grammar doesn't tell you that much about semantics 
	and then to "sat on the mat" as a verb phrase 
	and together this whole thing is a sentence 
	that is sort of the standard way of defining the parsing tasks 
	we get these discrete tree structures and they're different depending on the input 

7 (15) Learn Structure and Representation
	185
	00:07:10,430 --> 00:07:12,050

	of course where we will want is not just having this discrete structures 
	but ideally we want to have again everything represented as a vector 
	and so we'll actually look into how we can compute these vector representations that now represent any specific sub phrase without the context

8 (16) Learn Structure and Representation?
	192
	00:07:24,979 --> 00:07:28,160

	and so the first question we have to ask ourselves here is do we really need these structures 
	you know we're saying we learn structure and representation 
	but reading the last couple of lectures we've also learned some kind of structure and representation 
	it just happened to be that that representation was always everything to the left of what the current word is

9 (17) Sidenote: Recursive vs recurrent neural networks
	202
	00:07:45,950 --> 00:07:50,210

	and so that leads us to three slides of the side note of should we use these recursive structures the tree structures or just change structures and this could be a very lengthy argument 
	I'm going to try to distill it in just three slides 
	but basically the main difference here is that you know chains are actually special types of trees 
	right 
	they just happen to always branch in one direction 
	all right 
	so from general graph theory you know that changes are special types of trees 
	and so in that sense the recursive ones are just generalizations of recurrent ones 
	and the default could be to just say well I just take all the words and instead of looking at their grammar I just combine them one at a time right 
	and so you can see this as when you have these specific words you could just say I keep combining them one word to the right and that is also tree 
	it just happens to be able to flatten it out and call it a chain 
	in that sense they're actually not that different

10 (18) Sidenote: Are languages recursive?
	229
	00:08:58,880 --> 00:09:01,880

however the question is sort of then is language a recursive kind of thing 
	and so there's a long history here of papers 
	and I don't think it's yet clear whether this is cognitively plausible 
	like do people really put together specific phrases in their brains and so on 
	and fortunately we're not cognitive science class right now 
	so we can kind of put that on the table and say well, maybe, maybe not, but recursion is clearly helpful in describing natural language 
	it is very helpful to say for instance that "the church which has nice windows" is a noun phrase 
	I can replace that noun phrase with another noun phrase 
	they've got a grammatical sentence 
	and yet inside that noun phrase we have a relative clause "which has nice windows" 
	and that itself also contains a noun phrase namely the "nice windows" 
	and so that from just describing language is clearly very useful 
	and now there are basically four arguments of that we use for now 
	and basically say it is just kind of helpful for a lot of different tasks that we encounter in natural language processing 
	one of those is disambiguation so here I show you the first two parse trees that look a little more realistic something you would actually see when you parse the sentence 
	and you see here the the sentence you need read them from the left to right so here he eats spaghetti with a spoon 
	and the other sentences he eats spaghetti with meat 
	now what this tells us here is basically we have PE which is a personal pronoun and personal pronouns are noun phrases so far so good 
	we have verb phrase this year and now the entirety of that sentence eats spaghetti or that phrase eat spaghetti with a spoon is a verb phrase 
	eats is the verb now spaghetti is a plural noun 
	and now this is the interesting part here 
	this is a so-called PP or a prepositional phrase 
	and this overall problem here is a PP attached ambiguity 
	so if a computer was to read the sentence and you know reads he eats spaghetti with a spoon versus he eats spaghetti with meat 
	now how can we know whether the computer understands that when I say with a spoon I actually mean that I modify the way I eat the spaghetti 
	as in this PP should attach to the verb phrase versus I say he'd spaghetti with meat whereas this with which also just has some noun phrase inside of it this PP actually attaches to the type of spaghetti because we're not modifying how we were modifying what we'd  
	alright so if we have this kind of structure and we make this explicit it will be useful to disambiguate these two cases and now the model could actually you know tell us whether it got it right 
	and you know we can do downstream tasks now and also query for instance the model and ask give me all the different ways somebody could eat spaghetti alright maybe you did it with a fork maybe you do a fork and a spoon and so on 

11 (19) Is recursion useful?
	305
	00:12:10,640 --> 00:12:12,800

	are there any questions about this part? 
	state 
	you again 
	yeah 
	so back in undergrad 
	I actually took two entire semesters of just syntax and grammar theory and things like that 
	and so again this is like an insane oversimplification here could literally spend an entire year just talking about all the subtleties of how these trees are created so a bit of a cognitive overload in some ways 
	<вопрос>
	so the question is are they really useful for sort of semantic understanding versus just grammatical understanding 
	and the answer here is in some ways both 
	and a lot of linguists postulate that in order to understand the sentence you first understand the words 
	then you understand how words are put together 
	and then you know you get to the actual meaning of that sentence and you understand you get to the meaning through the structure first 
	and some people say well maybe not 
	all right 
	so this is this is a very much up for debate 
	clearly here we see you know a noun phrase well there's no there's no noun phrase here inside another noun phrase but here here that is the case right with meat the meat is a noun phrase in the spaghetti so we did understand more about grammatical structure here and certainly if we were to for instance train whether students are using correct grammar this seems like clearly a very useful model 
	however we also got some better understanding of what the model actually understood here when it read those sentences right 
	so eating spaghetti with a spoon and now here the model understood that with the Spooner you know understood 
	rotation marks 
	understood that with the spoon here actually attaches to the verb phrase and hence it modifies how you eat the spaghetti 
	so we did gain both syntactic and semantic understanding of this in 
	yes 
	individual earning offices so I did not what is the difference between what 
	that's a great question well we'll actually get to that a little bit throughout the lecture 

11 (19) -------------------------------
	358
	00:14:59,550 --> 00:15:02,759

	so the the next three reasons are sort of: let's say 
	recursive structure cognitively questionable 
	but clearly useful as a way to describe language 
	now it's also useful for just real tasks 
	so let's say we wanted to for instance do somewhat complex co-reference analysis where and koreff what we basically want to understand is when you refer today or it 
	what did you actually mean when you say that 
	right?
	and it usually means you have some anaphora for instance 
	so here's an example "john and jane went to a big festival. they enjoyed the trip and the music there"  
	now co-reference resolution would be the task of understanding who is they when I actually mentioned that this day right here 
	and in this case here would be John and Jane 
	so now we want to refer to that entire phrase 
	now when I say "they enjoy the trip" and I ask what do you mean by the "trip" 
	then ideally you would say well the trip meant "they went to a festival" 
	right?
	and so, now "going to a festival" is a specific type of trip 
	and it's not that "Jane and John went to the big festival" is the specific type of trip it's just going to a festival 
	so you might want to refer to just that and now somebody else could have that same trip and you can refer to that as one coherent unit 
	and now there would be potentially the big festival maybe there are two festivals a big one in small one they went to one not the other now you go basically there 
	so this year basically we've seen a couple of times 
	we could also refer to just her and just him for instance just Jane and just John and in both cases we wouldn't want to have only a representation for everything that was read until now 
	right?
	we would only want to have something we can refer to that is a sub phrase in that sentence 

	third reason is that the labeling sometimes becomes less clear 
	so if we basically only have a single label at each word
	so here we have a sentence such as "I like the bright screen but not the buggy slow keyboard of the phone" 
	now ideally we could just classify buggy so keyboard of the phone or buggy slow keyboard by itself as an entity and that is negative whereas something before here is positive 
	right?
	and we don't necessarily just want to classify the entire sentence here or this entire paragraph 
	but we ideally understand each of these sub phrases and they can have different kinds of labels if we were to classify sentiment for instance 
	and again here it is an interesting co-reference problem 
	it was a pain to type with or it was nice to look at depending on what the information here is the semantics of what's following the it it's actually either referring to the screen or the keyboard 
	so this is the task of coroner's resolution it's actually kind of an interesting task and NLP 
	alright? 

	and then the last argument is somewhat of a pragmatic argument which is in some tasks it just works better to use these grammatical structures 
	but this is also an ongoing field of research so maybe eventually we'll find that we could get away which is some very very deep lstm model and we don't need them at all 
	and maybe whenever you have a phrase you can kind of have some neurons that kind of just capture now as a phrase and then the forget gate turns on and and they are the next layer will kind of deal with this thing 
	it's still up for debate and it's a very active area of research 
	but, okay, let's for now assume and it's still the case that on some tasks these models work the best now on some standard benchmark data sets 

12 (20) Recursive Neural Networks for Structure Prediction
	450
	00:18:41,730 --> 00:18:43,740

	and so let's define actually what a recursive neural network is and how we get these parse tree structures 
	so basically we'll have two inputs which are in general the candidate children's representations 
	so in the beginning those will just be the words for instance "the" and "mat" and then later on it could be representations of phrase vectors that actually already were computed before 
	and the output in the first kind of example where we actually look at how to compute the tree structure will be two things 
	the first one is the semantic representation if we merge these two nodes 
	and the second one will be a score of how plausible the new node would be 
	so ideally here again we'll compute a score

13 (21) Recursive Neural Network Definition
	469
	00:19:27,530 --> 00:19:30,570

	that will say this is a reasonable syntactic phrase 
	and we want to increase that score if it was actually true and we want to decrease it if it's some random ungrammatical phrase 
	so what are the equations for this model here 
	well, it's a very simple one: the first thing we do here is we just concatenate c1 and c2, the children, the left child and the right child of each node in the tree 
	we concatenate them and then we multiply them by this matrix W here 
	and now the parent vector P should have the same dimensionality as each of the single children 
	so let me ask you what should the dimensionality now of W be 
	let's say c1 and c2 each one is N dimensional. it's an N dimensional vector, now what should the dimensionality of W be? 
	say louder 
	yes 
	exactly N by 2N 
	and so to understand this a little bit we can also write this in a different way we could instead of writing W times c1 and c2 
	we could rewrite this and say well W is a block w1 w2 so like we said this is an R N by 2N matrix 
	and this matrix will be exactly this and we have here C1 and C2 as our vector representation 
	and now this is actually equal to W1 C1 plus W2 C2 
	all right 
	and that looks very similar to various equations we've had in recurrent neural networks to where one was the history of that we had in the past H_{t-1} and another one is X_1 the next word 
	now it just turns out that C could be both it could be single words or it could be hidden dimensions 
	and we'll go through an example here in the next couple of slides 
	so the reason we call this recursive and before recurrent is that we use a also here the same W parameters at all the nodes of the tree 
	so in order to compute any one of these here we'll share the weight 
	so we tie the weights and basically have the same W here to compute every single parent vector 
	yes 
	that is correct 
	yeah 
	the word this model is not it really depends on the order of the words 
	okay 
	so this is the main equation and again we see here this is really essentially just a standard single layer neural network 
	just happens to have as input two vectors that we concatenate which the model doesn't really know 
	so the model is just a single vector but the way we put them together is by taking one vector from the left child and one vector from the right child 
	so the central equation of of today 

14 (22) Parsing a sentence with an RNN
	532
	00:22:48,350 --> 00:22:51,740

	and so now how do we actually use this to compute a tree structure that is captures the grammar of that sentence 
	we basically and this is the first simple version of how to describe this and basically the only one we'll describe in detail today so called greedy search mechanism here 
	we just basically look at how likely would it be to combine the vector of "the" and the vector of "cat" with this neural network 
	and that would get some kind of vector representation first and then a score that was just a linear layer 
	so again here the score which we kind of skipped is just a simple inner product same kind of score that we used before 
	so now we look at this and then we'll just look at every possible pair of words and how likely they would be to combine to a reasonable syntactic phrase 
	and now it's just in gramatic structure and according to linguists on the is not a reasonable syntactic phrase 
	but "the mat" is a nice proper noun phrase so that would get a large score 
	and likewise "the cat" is a very obvious very simple combination of a determinant a noun so we put those together 

15 (23) Parsing a sentence
	560
	00:24:02,840 --> 00:24:05,660

	so now "the cat" here gets the higher score 
	and in this kind of greedy search mechanism we will just say: all right, well, then, let's combine those two 
	let's instead of having these two separate word vectors here we will now just take the phrase vector which is really just the output of the hidden layer 
	so now the model basically has a one vector here and that's "the" first vector of the sequence and that vector represents now "the cat" 
	and now we have basically have here "the cat" has a vector and then still a "sat on the mat" as each one being a separate word vector 
	and now the model doesn't yet know how likely would it be to combine "the cat sat" 
	and so this is where the recursion comes in the output of the first time we apply it the neural network becomes the input to that same neural network and that's why it's recursive 
	so now "the cat sat" is somewhat of a reasonable phrase we could just say in "the cat sat" instead of if "the cat" stands 
	and so this would get a reasonably large score 
	however "the mat" was an even more obvious noun phrase 

16 (24) Parsing a sentence
	588
	00:25:21,900 --> 00:25:26,280

	and so we'll merge those two nodes next 
	and now again the model doesn't know how likely it would be to combine the word vector "on" with the phrase vector "the mat" 
	and so we'll again apply the same neural network 
	yes 
	it's a great question 
	it goes a little bit too far so I can't go into too many details 
	but for those of you who actually have taken the to 24 and no CQ I see you okay it's just three names parsing and chart parsing 
	in chart parsing you can actually make certain independence assumptions kind of like you make markov assumptions for language models 
	we say: I don't care about anything else that came before but right now this phrase is a noun phrase 
	and because of that you can then say: well, I can find the global optimum here 
	because I can make certain simplifying assumptions use dynamic programming here you can't really use that because the vectors on which you make those assumptions are continuous 
	and so really changing the vector just a little bit would change potentially the overall score 
	so while one phrase might be locally the best highest-scoring phrase it might not actually do to the global optimum 
	so all you could do here is a so-called dynamic beam search but don't worry if you don't weren't able to follow that little sidenote 
	so in the end what we'll do is a CKY like beam search but again don't worry about this it's enough to understand 
	so this greedy procedure here for finding the best tree

17 (25) Parsing a sentence
	626
	00:27:08,679 --> 00:27:10,659

	okay so now we basically keep building up this tree find the highest-scoring potential candidate and in the end have the full tree structure 
	yes 
	sorry can you speak a little louder 
	that's a so there are certain constraints on these two that they have to be symmetric 
	that's actually an interesting question 
	there are none so right now they can just be any arbitrary matrix 
	and it turns out that there are interesting extensions that we'll go into later in the lecture that actually allow us to really understand a little bit better how each what the role of they are of these two blocks are their left block and the right block 
	well, we'll get to it in a couple slides 
	but yeah in general it's just a W matrix it will be updated with SGD or other kinds of optimization methods and there's no no way to sort of force and force kind of seventeen you don't want to enforce symmetry because maybe on the left the left phrase or left word might be more important than the right one 
	and any other questions on how this sort of greedy search procedure happened 

	yet so yes 
	the same neural network here was used for every single node computation 

	alright 
	so the question I guess is it's a little fuzzy is it okay 
	to do this well it turns out it's okay for tasks for certain tasks 
	yeah 
	it's good enough to get these day of the art performance on them 
	yeah 
	so the question is it good to have different kinds of neural networks and actually relax the assumptions 
	so the way I describe this so far is that these kinds of models actually more recursive than actual natural language 
	natural language tests sometimes a noun phrase inside a relative clause inside a noun phrase 
	here we use the exact same neural network at every single node of the tree 
	and so we'll actually relax that assumption later on the lecture 

18 (26) Max-Margin Framework - Details
	679
	00:30:10,800 --> 00:30:12,720

	so how do we actually train this 
	we will use a similar max margin objective function to what we had used before in one of the earlier lectures 
	but now we'll define the score of the entire tree 
	and we don't just have a single window we have a tree that actually was computed from multiple different kinds of scores 
	and so the score of the full tree is for us right now simply the sum of all the scores at each node 
	so let's define here the score of a specific sentence X with a specific tree Y 
	we basically look at all the nodes that tree had and we compute the scores at each of these nodes 
	and again these scores here were just simple inner products with the same view vector 

19 (27) Max-Margin Framework - Details
	697
	00:31:01,710 --> 00:31:03,240

	so here comes the the most important and kind of interesting new equation of of today's lecture basically this is called max margin parsing and we have here this max margin objective function so let's walk through this objective function very slowly what we want to do we want to generally maximize this function and we assume we have a labeled training data set so we assume a bunch of linguists sat down and said for every sentence X I I have I give you the correct tree why I and now this again here was just the sum of all the nodes of those trees and that sum was just a sum of a bunch of inner products right so again when you think about you know how would you train this model it's just a bunch of gradients of you know a sum of inner products underneath which you have a neural network okay so that that's easy but now if we just maximize all the scores of all the correct trees well we could just everything would go to infinity right we just maximize everything well we actually have to tell the model is how to deal with finding the right tree by itself and this is what the second part here is so we're going to try to find the maximum over this set and the set a of X I is the set of all possible trees that you could construct from X I and so there if you're familiar with with combinatorics they're Catalan many potential binary trees so it's exponentially many possible binary trees you can actually compute that number and blows up very very quickly it's an exponentially many number of trees so this is where we will have to find some smarter way to go through them and find the highest-scoring one and the simplest one that I described was in basically this greedy procedure here we just take the highest-scoring current set and hope that that will lead us eventually to the highest-scoring tree but of course that isn't necessarily true here we found the correct tree but maybe the model incorrectly would have said cats at is the highest-scoring one and then we couldn't recover that later on right so this is basically a search procedure and they're different ways to do search and and in some ways if you you know interested in kind of the search aspect of machine learning and they I then I think 221 is the right kind of lecture takes sort of an introduction to AI different kind of search strategies all right so let's assume for now that we find the highest-scoring one simply with this greedy procedure and we basically you know do exactly what I described the previous slides and now we define one specific Y that is the maximum of all the trees which we could build and now because of this my here we're going to try to minimize the score of that highest-scoring tree and let's assume let's ignore this part for now if the highest-scoring tree that we're now minimizing actually happens to be the exact correct tree well then we're done here right then why I and this y are actually the same these two cancel out and we're done and now this is where the interesting margin penalty comes in which as indeed actually an important part of this objective function and this margin here essentially penalizes every incorrect decision that you have made so if you incorrectly combine two words then you add one to that tree structure and what that the result of that will be that once you actually get the correct tree here as Y and y actually is weii then that Y will have a high score that is higher up to a certain margin to the next scoring but incorrect tree let's parse that pretty complex sentence so essentially what this Delta here does is it encourages the model to make mistakes it will add a bonus point to every mistake the model makes and once it goes past that bonus it actually makes the right decision with a margin of Delta which you usually just for every incorrect decision we set it to one so to score for the right decision will that new one larger than the score of an incorrect decision so in the beginning here we encourage the model to basically do the worst possible thing all right and then model makes all these mistakes and then you can tell them well these were all wrong and you minimize all the scores of these incorrect decisions and that's that's where yeah that's sort of the idea of the next margin loss are there any questions about this yeah so in almost every single weight in all these different loss functions that I always describe has a standard l2 penalty and all the weights the Delta yeah the Delta does not it just basically encourages the model to make mistakes during the search procedure it doesn't actually back up something in itself great question yes I okay so again X I is the sentence so in this case you know your cat sat on the mat Y is the correct tree structure basically linguists sat down went through a lot of sentences in most cases so-called penn treebank and in the pantry bank they said all right for this sentence this is the correct grammatical analysis of that sentence all right yes can one turn be negative yes so these scores here just inner products right inner products of real numbers so they can be very negative 

20 (28) Backpropagation Through Structure
	834
	00:37:17,170 --> 00:37:19,990

	all right so how do we actually train this kind of model and what does it what does this look like again in some ways you could just do this right away will require a lot of thought I little embarrassing but I actually had reinvented this whole thing I didn't know it was it existed I didn't have a name for what I was doing so I actually had reinvented this whole thing and only later I was like I'm gonna call the recursive one recursive known that or it's the best thing ever and then I you know a couple days later I'm like I should Google recursive neural network and and of course some people had invented it in the past but have a more emotional connection now since I had reinvented the wheel a little bit so this is really just standard like taking derivatives again but there's you know they're cut a couple of subtleties here so let's walk through those quickly the main equations that we had derived in their gory details and you know you can read up in a lecture notes now to again we have these Delta's the error messages that come from every layer and they're basically being passed down the neural network and we will update the different W matrices here with this outer product of the delta from the previous layer times the activations of the current layer so those equations will still be the same but there are a couple of couple of subtleties here namely the following three differences that result from having now the structured object instead of just the same chain that goes you know and has different weights at every single layer of the network 

21 (29) BTS: 1) Sum derivatives of all nodes
	875
	00:38:48,550 --> 00:38:51,190

	so let's walk through slowly through through these differences so the first one is we can actually just sum up all the derivatives at all the nodes which initially might not be that intuitive right because in theory this is a function of W which is a function of W which is a function of W and so on goes down the tree so just to give you an intuition here of how this comes about I basically just wrote down all the gory details for a very very simple sort of recurrent function it's the same for recursion so let's assume here it's just a tree that happens to be a chain it just goes up and up and up and there's only one input X and you could even think through this as just being single numbers W is just a single number X is just a single number and you know we just take the derivative with respect to W now we can basically write that out and assuming W here is the same we'll you know apply the chain rule but now you know when you take derivative of a function in multiplication you know have to take derivative of both sides if their parameters inside and this is really basic algebra you basically end up with this kind of equation and now if we assumed that instead of having the same W here you actually have a different one this is W 2 and this is W 1 so assuming that you had a different W at all the different nodes in the tree and in this case even simpler than just a simple function you actually end up with something that looks exactly the same if you're in the end just get rid of the indices again so this is fairly straightforward you can just write down the derivatives by yourselves and you'll arrive at basically this kind of property 

22 (30) BTS: 2) Split derivatives at each node
	920
	00:40:39,080 --> 00:40:42,140

	so really all we need to do is we go up to tree once compute all our hidden activations just like in forward prop of any other neural network and then we have to go down to tree once and as we go down the main subtlety here is now that because we concatenated the two children vectors we have to somehow split our error messages as we go down so what do I mean by this during forward propagation here the parent is essentially computed using those two child vectors that we had concatenated and that means that when we now compute out deltas our error messages again with the similar the same kinds of equations that we have here then we will essentially just assume that the Delta that comes from that parent and is sent to the two children it's just basic a vector that we can now also split the same way we concatenate it the children we can split the error messages from one side to the other  all right and now the last part is that we have scores at every node but every node is also part of the score computation of another node namely the parent nodes right so we have here c1 and c2 they have a score and we want to minimize or maximize that score depending on whether that phrase is in the right or a wrong parse tree but now their parent will be the left child in this case in this kind of visualization here the left child of yet another combination and so when we take the derivatives with respect to W here of the parent that will basically send messages all the way down to the leaf nodes every node of the tree sends error messages down all to all its children in the tree and so just as before if we have two different objective functions on top of the same layer we basically just add the error messages coming from both so we have here 1 Delta message that comes from the score that we compute and another Delta message that comes from the parent that uses this for computing another score 

24 (32) BTS Python Code: forwardProp
	971
	00:42:58,880 --> 00:43:01,819

	and you'll actually have to implement this in your last problem set either this or convolutional neural network which we'll go over next week even more complicated so it's very important to understand this and I'll give you a little bit of a hint here for the last problem set largely because I want you to be able to solve the problem set 3 quickly so you can have a really cool epic class project which is very important and I think of the most interesting last thing so the output for most of you from this class so here we basically walked slowly through some Python code for the forward propagation and just to show you how similar this is to all the other neural networks that you already have with these three subtle differences so you have some kind of recursion that you can implement there are different ways you can implement recursion in tree structures but basically to compute the hidden activation for a specific No we just basically sack here these to the left child and the right child and their hidden activations which in the beginning again could just be war vectors and then we just basically have a dot product or you know multiplied this w matrix times this concatenated child vector so that's fairly straightforward and then we add the bias term here and then we apply our element wise non-linearity which in this case could be the rail you could be ten h2 and so on alright so that's basically the equation of the recursive neural network that I put into a red box and then we just compute for instance a score or in this case here a softmax probability again or compute the softmax to get a probability for some kind of class so we could assume WS for instance are here our soft mix weights and you've already implemented this as well and one interesting side note here sometimes you will actually run into stability issues when you implement these kinds of models especially for rectified linear units your numbers can get very large and so this is an interesting sort of trick or hack to prevent overflow and your floating-point operations which is you just subtract the maximum from those numbers and you should verify that this doesn't really change the probabilities and then we take the exponent of this sum over the exponents and that is basically the probability here for any kind of class for any kind of node so what does that mean we can now basically classify that you know the keyboard is a neutral phrase for instance and sentiment and then the buggy keyboard is you know a negative phrase and buggy keyboard which I really really hate it when I have to type on it is now a riri negative phrase right and so you can basically classify all the phrases as you combine different words differently and this will eventually lead to being able to really classify that I this movie didn't care about cleverness wit or any other kind of intelligent humor while having a bunch of phrases that are really positive at the end at the inn somewhere in the beginning it says it doesn't care about it and then it becomes negative so this is basically what this kind of model can do

25 (33) BTS Python Code: backProp
	1052
	00:46:08,580 --> 00:46:09,930

	all right now comes the interesting part which is the backdrop so I mentioned there are three three different things here so let's walk let's walk through this here we have the deltas which just comes from the softmax so you've already all derived that in all the gory details and you've implemented it so this one should be fairly straightforward of how you compute here your deltas and again here you have an interesting you know W transpose times Delta that's the first one now here comes the interesting settled notion of having a delta potentially from the parent node as well as from your own prediction and if you have a parent which basically every internal tree does then you will just add your two Delta's the Delta that you have from your own softmax or your own inner product for your scoring function plus the error that comes from your parents so this is also recursive kind of function and then we take our element wise non-linearity here f prime of Z and then then it's fairly straightforward if you are a leave vector and this is actually an interesting notion here too you can train your word vectors as part of the entire recursive neural network model so in some cases if your data set is so large for what you actually care about you don't even have to run work to Veck or Glove or any other kind of word vector separately you can just train a word vectors as part of the model again everything is a parameter the word vectors are parameter you just add them and then as you go through the tree and you send down different error messages and whatever kind of tree you know you have Delta message coming from here send down to both of the both of the children keep sending it down at the end they arrive at the word vectors so why not tell the word vectors well if you just changed around a little bit in your vector space you might actually do a better job in that classification somewhere higher up in the tree and that pushes the word vectors to be somewhere else so basically you can collect here all your derivatives for your word vectors to just taking the Delta that has arrived now at the word vectors and if you're not a leaf node if you're not at the very bottom of the tree but you're somewhere an internal node then comes in the standard stuff here we have the derivative of W will just be Delta outer product with the current activation the current activation in this case was the left child and the right child so we just concatenate those again and get exactly here the main equation we had derived before just an outer product of the delta from the previous layer the outer product with the activation at that current layer and same with WP here we had there so just you add the deltas to the derivative of B and now you compute the next Delta to go down one layer with this equation we had derived which is just W transposed times your current Delta and now you send those to the two children and eventually the F prime here again will happen at that node all right so this hopefully will help you a lot in in problem set three but you'll actually have to derive it and you know I didn't add all the details and you have to really understand how to send around Delta's in a different kind of tree structure so still still a lot of work so don't don't wait for too long and thinking this is the entire thing yes oh this one here is an Audemars product or an element-wise multiplication

26 (34) BTS: Optimization
	1141
	00:49:56,240 --> 00:49:59,880

	all right so now we have all our derivatives and we basically do the standard stuff which is we take some off-the-shelf optimizer such as SGD which we had used can also use l-bfgs in most most cases you know standard optimizers and matlab or Python actually have both of these as options we had also mentioned before to actually use a degrade and this is also very useful here for for our model so we can actually update here again our theta at the next time step of the optimization by dividing over the gradients of all the previous time steps we mentioned this in one of the tips and tricks in a previous lecture and it turns out this is also a subtle difference here that you don't we don't really have to go into too many details with but because you have here non continuous objective function imagine you change the highest-scoring tree and now all the sudden because of one little subtle difference in the score at a lower layer and this greedy search procedure now it's a completely different tree now this is a you know non continuity but it turns out you just still take your derivatives and I use sup gradient so what this means in practice is will basically compute the score here of the correct tree maximize all those take derivatives with respect to all the parameters and increase all those scores find here the highest-scoring negative tree with our search procedure the highest-scoring incorrect tree sorry and then we just minimize all those scores so they're just two trees at every time step one tree is the correct one we maximize it scores one tree is the highest-scoring incorrect one minimize dos once they're both exactly the same well then those two terms cancel each other out and you're done with that tree and you can ignore it and only now try to optimize over trees you haven't yet correctly parsed

27 (35) Discussion: Simple RNN
	1191
	00:51:49,910 --> 00:51:54,090

	alright so that was the most basic definition of a single matrix recursive neural network and those actually obtain reasonably good results and I had a bunch of papers or state-of-the-art results and different tasks I'll actually cover some of those in the next lecture but one interesting problem of them and this is similar to the recurrent neural networks where we had the same W at every single node to is that they can't really capture all the complex phenomena of how you should compute and map now all these different phrases into the same vector space it's a pretty complex procedure we're asking here a single W which essentially boils down to just you know some affine transformation that you you know add to two child vectors we asked it to really capture all the complexities of how to create meaning in a language right it's it's a little too much to ask of a single matrix W were essentially asking it to combine different syntactic categories I have an adjective here and a noun here and I multiply it the first part of my W matrix times the adjective vector and then the right part times the noun vector but now you know the next layer I might have a verb phrase plus now that combined noun phrase and now I take that same W matrix and multiply it with a verb and I expect that transformation to also map it into a reasonable part of the vector space it's a lot to ask of a single model or single W matrix 

28 (36) Solution: Syntactically-Untied RNN
	1229
	00:53:26,320 --> 00:53:29,170

	and so the idea here is that because we're already in grammar land and we already have these syntactic tree structures why not use them a little more and essentially condition these composition functions these neural network layers that have only a single W and instead untie the weights relax the assumption this comes back to a question that was asked before which is should we really use the same W here at every single node of the tree and what we could do instead is actually condition these composition functions on what kinds of syntactic categories they're actually combining and this is this an interesting notion here so let's walk through an example so here this is the standard recursive neural network where we use the same W here at every single node in a tree and now let's assume we have actually some knowledge about what these word vectors are and this knowledge comes in the form of discrete categories so let's say a B and C here capital letters are actual discrete categories so maybe this is you know I so it's a personal pronoun and we know this vector here is a personal pronoun like is a verb and cats is a noun and so see here is this is a noun phrase B here is this is a verb and now we'll actually use a W matrix here that we select different W matrices we have a large set of them usually around 80 or so and in depending on what syntactic categories you have to use different W so here we have the first W that combines syntactic categories where the left child is of the syntactic category B and the right child is of syntactic category C 

29 (37) Solution: Compositional Vector Grammars
	1271
	00:55:12,700 --> 00:55:17,350

	yes right so how do you get the syntactic categories basically you use a very simple model that is not very accurate but basically just looks at the summary statistics of your corpus your training corpus you know what the right trees are all right the linguists again had sat down created for instance penn treebank an infantry bank you can know all right these kinds of things only ever combine this kind of way and just taking those counts basically boils down to probabilistic context-free grammar and we can't really go into the details in this in this class but it's a very simple generative model of of parse trees and in its most simple form is very fast but it's not very accurate but it turns out it's good enough to give us a bunch of candidates which we can then go over and it's also good enough to help us condition and get syntactic categories yes you we will actually compute this final score as the sum of the recursive no net or score as well as log likelihood from the pcfg but you we're not going to weight them but we will get some weighting coming from now having different composition matrices W and I'll show you in a second what these look like yes it's a good question I think you actually have the option here to also use a deeper network you could also just classify each of these vectors here and it's separate there's a separate neural net we're going to say alright I classify this as a verb phrase if I said this is a noun phrase and then I'd condition my my composition function based on these classification problems it's actually also a reasonable option

29 (37) Solution: Compositional Vector Grammars
	1313
	00:57:05,390 --> 00:57:10,850

	all right so basically for yes it's a great question so we will actually in the simplest case just assume let's say we have a model that gives us for each sentence 200 candidates where we know the syntactic categories of every single phrase and word and this in our case will be a very simple fast pcfg model don't worry if you don't really understand PCF jeez if you've taken 224 good for you if not let's just assume you have some Oracle that gives you for each sentence 200 or so candidates of what could the right structure be and ideally you know in over 95% of the cases one of those 200 is actually the right one it just turns out the model that gives them to you doesn't actually know which one is the right one and so it helps us basically to prune very unlikely candidates which is just a speed hack we could use a neural network but then you have lots of matrix multiplications in sight to compute all these potential candidates and this one helps you and it also provides you some core syntactic categories of the children 

29 (37) Solution: Compositional Vector Grammars
	1342
	00:58:23,160 --> 00:58:25,039

	and so we will call this full model here compositional vector grammar basically combines a simple model like a pcfg with a more complex recursive neural network 

31 (39) Related work for recursive neural networks
	1346
	00:58:33,329 --> 00:58:36,960

	there's been a bunch of work on these and in most cases it's quite different they assume fix tree structures they didn't actually do next margin learning they didn't actually use a large large trees and long sentences

32 (40) Related Work for parsing
	1351
	00:58:46,319 --> 00:58:48,119

	and so on there's also been a lot of work in parsing where people have basically taken the manual feature engineering approach and try to improve the original simple pcfg where you just say this is a noun phrase and without going into too many details here you can basically look at all the errors you're making and try to describe each of the categories with some richer but still discrete representation so in some ways and sort of historically this kind of model extends all these other ideas from taking counts over discrete kinds of information to just representing everything as a non discrete as a continuous vector representation 

33 (41) Experiments
	1368
	00:59:27,869 --> 00:59:29,220

	so the numbers here are in some ways not that interesting there's lots of work on this task a lot of researchers and natural language processing care about parsing and so there's been lots of different different work here what's interesting here is if we have the same weight matrix W at every node of the tree we actually don't get a very competitive parser but once we allowed the syntactic untying having different W matrices we actually get a very very good parser there's still other ones that take into consideration even more external data but they're much much slower at both training and testing than in this one and the way we measure this years again with f1 where we have precision and recall over the various nodes in the 

34 (42) SU-RNN Analysis
	1388
	01:00:17,640 --> 01:00:20,460

	tree what's more interesting is that linguists had a certain notion of soft edward's in fact one of the baseline models here from Collins tries to lexical eyes these parsing decisions and say well this is a noun phrase with a cat inside for instance or in this verb phrase here may be the verb matters more than a noun because in the end the action sort of matters more than maybe the things that depend on the action what's cool is this model actually learned what linguists had themselves sort of come up with by trying to analyze grammatical structure in language so it for instance learned that when I combine a determiner like the or a with a noun phrase like cat or Church or Windows that the noun phrase actually matters more so what do we see here this is essentially exactly this kind of matrix the left block of is the matrix that is multiplied with the left vector the right block is the block that is multiplied with the right child of that phrase and now here we basically just visualize how large our elements are and I had brought up in a previous lecture that there's sort of this hack of initializing these matrices with identity matrices and that's exactly what we did here too we basically have we start by having to block identity matrices so the default in the very beginning of the model if you don't know anything is actually just average to - yeah you start here with 1/2 times these 2 identity matrices so you have here 1/2 times and any matrix basically just comes down to 1/2 C plus C 1 plus 1/2 of C 2 so and then you know of course you apply to non-linearity but in general when you start out with that's what you get and then slowly but surely the model pushes itself to basically start to ignore more and more with its deck hat or a cat or this cat or that cat literally and just realize ok the most important thing here for the semantics of the resulting phrase is that it's a cat not that it is that or this array and that's that's really neat right the model here learned something that linguists had also come up with through many years of research but a model learned it just from looking at lots of data 

35 (43) Analysis of resulting vector representations
	1445
	01:02:45,250 --> 01:02:47,950

	what's more interesting is we can actually now look at what these word vectors and phrase vectors are capturing so we can look at nearest neighbors in the phrase vector space and just like report vectors the word vector space is basically captured syntactic and semantic information nouns were closer to other nouns and then once you zoom in the semantically similar nouns are closer to other semantically similar nouns the same thing it's kind of true for phrases it's of course much harder because in general for most sentences and most reasonably sized score Prada it might not be that many other sentences that are very very similar to that sentence this is a much more complex space and a lot of NOP researchers had actually initially argued that it's almost ridiculous to squeeze a variable length sentence structure into a fixed vector representation I think now it's becoming more and more common and with the results from machine translation that we covered in the last last week's lecture we actually see that well it is a reasonable enough representation to really capture a lot of subtle information so here what we did is we he cooked the top vector of one of these sentences and we mapped a bunch of different sentences in this case from the penn treebank into this vector space and then we pick one sentence and then we look at the nearest neighbors of that sentence in the resulting vector space and for the sentence all the figures are adjusted for seasonal variations the closest sentence to that one is all the numbers are adjusted for seasonal fluctuations so here it basically just learned well if I have two similar word vectors variations and fluctuations and figures and numbers but the rest of the tree structure is the same well that you know still keeps them the same so that's maybe not that impressive right but the next one is interesting all the figures are adjusted to remove usual seasonal patterns so here it actually learned that to remove usual seasonal patterns it's kind of similar to seasonal fluctuations or adjusting for those and so it actually had some sort of invariance to adjusting here a hole verb phrase versus adjusting just directly this noun phrase and knight-ridder a bunch of organizations wouldn't comment on the offer the model learned that that's somewhat similar to how score declining to say what country plates the order or coastal wouldn't disclose the terms so it learned here that you know two wouldn't comment and decline to say and wouldn't disclose are similar kinds of things that companies would do and personally I was very excited when when I saw these kinds of results in 2000 2010 and I was one of the reasons I sort of went into you know deeper and deeper into deep learning because it's kind of learned some interesting patterns that we didn't explicitly tell the model to try to capture and basically was also the first step into thinking about applying these models to paraphrase detection or understanding semantic similarity between different sentences and for those of you or think about you know summarization this is also an important kind of result for sort of sentence summarization not including the same sentence as multiple times

36 (44) SU-RNN Analysis
	1530
	01:06:07,390 --> 01:06:09,520

	another interesting one was exactly this example that I brought up from from the beginning with the entity the PP attachment ambiguity so the prepositional phrases and where they should attach should they attach to nouns or the verbs so here the question is can we transfer semantic information from a single related example so here the idea is we add two training sentences and then we test on these two test sentences and these all these four sentences are actually incorrectly parsed when you take any or most of the standard parsers because they were usually trained on a penn treebank which comes from the Wall Street Journal and The Wall Street Journal just doesn't talk much about eating spaghetti so they're almost all incorrect or all for most parses and then we basically add these and now because we use word vectors to make these decisions we don't need to see exactly spoon and meat at training time because at sorry a test time because a training time we saw fork and from our word vectors we know that fork and spoon are actually two similar kinds of things they have similar vector representations so the hope here is because we don't think of these as discrete entities and we just keep counts in a probabilistic model and we keep counts for fork and now well you know if you'd never seen spoon before your counts for for fork don't really help you if however you learn the neural network model that takes into input takes as input something that looks like a vector in this area of the space and you find something else that is in that similar space such as you know fork and spoon having closed word vectors 

37 (45) SU-RNN Analysis
	1573
	01:07:42,040 --> 01:07:43,630

	then there's some chance that this will happen and that's exactly what did happen which is we had here the original Stanford factor parser which didn't get the PP e attachments correct and the compositional vector grammar I described before correctly put the prepositional phrase here with a spoon to the verb phrase and with meat to the spaghetti yes sorry sagen how are the children is my uh-huh mm-hmm oh this one right here so the question is how did we actually get from the binary tree structures that I described to ternary ones it turns out any grammar and Chomsky normal form can actually binarized and then you can have a deterministic procedure to get back to basically map between a binary version and one that isn't binarized and still recover the exact same grammar and so this is basically a deterministic sort of post-processing step to then create create ternary structures you could also in the greedy search procedure actually have a double u that just has three blocks W 1 W 2 and W 3 and you just multiply it with three word vectors and compute you know again a parent vector P for that but it just turns out that if you try to use CQI like chart parsing then that blows up your complexity even more so you rather just do it as a post-processing step don't worry if you didn't understand all that things I just

38 (46) Labeling in Recursive Neural Networks
	1608
	01:09:39,839 --> 01:09:44,259

	okay so I showed you in the code that we can use these parent vectors also to compute any kind of class right we can predict sentiment for every phrase we can predict the syntactic categories lots of different things we can predict and we will do that with the softmax classifier at every node and again we train it similarly to before and in the case of the actual parser we can actually train this by having two scores and we just add the two errors the max margin error we add the Cross enter pair and we just take derivatives with respect to both same way we did before

39 (47) Scene Parsing
	1623
	01:10:18,750 --> 01:10:23,440

	so now let's think about scene parsing it's actually in some ways and this is kind of a fun side note for the last three minutes that shows you that once you are really good at deep learning for natural language processing you really now have a tool in your bag that lets you do almost anything that is related to data because almost every data can be represented as some kind of vector and you can use these kinds of technique for lots of other things so last little three-minute side note here on computer vision where you could kind of assume that even in scene images you have a similar principle of compositionality going on where you can define the meaning of the scene as also a function of the smaller regions you know there's a bush there's some people there's a you know a roof they're a bunch of different objects here or parts of a larger structure you have a tree now the single parts of the structure composed to form a building and you know then all these various objects interact in certain ways

40 (48) Algorithm for Parsing Images
	1649
	01:11:26,350 --> 01:11:30,910

	and it turns out that I don't need to actually describe to you in many details how you would learn how to combine little regions because it's the exact same neural network that we just described for sentence parsing we basically the main difference here is instead of starting reward vectors for words we actually start with feature vectors for little regions and images there's some computer vision technology and how to actually compute those and program that but let's assume we have a vector a presentation that captures features of small image regions well now we use that exact same neural network we can have you know a bunch of neighbors now it's not just the left word and the right word but all the regions that are neighboring a certain other region and image we can compute scores for whether we should combine them or not we have some training data for that that says you know this part all these different regions are actually part of the same you know building or our street or group of people or three or whatever and then we lower we increase scores for things we want to combine we decrease scores for things we don't want to combine and that way we can build these whole structures 

41 (49) Multi-class segmentation
	1681
	01:12:37,420 --> 01:12:41,410

	and this kind of model actually you know back in 2011 was a state of the art model since then actually people have have come up with even better models all of which I think are also deep learning based but here you can basically see you know what kinds of things that labels as Street or trees or buildings and so on and so so yeah hopefully that shows you that really the kinds of tools you learn here and we've already seen this for some class projects some of you are using these tools to understand genetic language instead of natural language but really it's a very general set of tools that that you're learning here alright and that's it for today



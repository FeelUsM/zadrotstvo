1 (1) Лекция 9: (завершаем: LSTM и) Recursive Neural Networks
	1
	00:00:00,030 --> 00:00:03,480

	девятая лекция CS 224 d 
	сегодня мы поговорим о рекурсивных нейронных сетях
	У меня есть некоторые личные отношения с ними, так как я работал много таких моделях во время моей кандидатской диссертации

2(2) Обзор
	6
	00:00:11,610 --> 00:00:14,160

	так что мы действительно проведем две лекции по этой теме
	так же, как рекуррентные нейронные сети они очень важны 
	это очень гибкие виды моделей и поэтому в основном сегодня (?) мы будем мотивироваться так называемым композиционным значением 
	вы знаете, своего рода метод, чтобы добраться до смысла длинных фраз 
	и тогда мы на самом деле посмотрим на немного другой вид целевой функции, вид прогнозирования структуры целевой функции, 
	и обратное распространение через структуру действительно будет очень похоже на обратное распространение, которое вы уже знаете, только с тремя небольшими изменениями
	приятно, что эти модели очень общие 
	если у нас будет достаточно времени в конце, мы также можем посмотреть на два или три слайда на примере компьютерного зрения, 
	оказывается, та же самая модель будет работать и для изображений 
	в следующей лекции мы можем пойти и посмотреть на пару модификаций, пару расширений к стандартным рекурсивным нейронным сетям 
	когда я упоминаю RNN сегодня, это будут в основном рекурсивные нейронные сети
	потому что последние пару лекций были про рекуррентные нейронные сети
	и мы действительно рассмотрим разницу между этими двумя типами

3 (11) создание моделей над пространством word-векторов
	39
	00:01:21,540 --> 00:01:24,990

	мы уже хорошо знакомы с моделями векторного пространства word
	где слова, которые похожи по смыслу и имеют похожие виды частей речевых тегов находятся рядом, поэтому все существительные сгруппированы вместе, а внутри кластера существительных будут иметь разные семантические подструктуры 
	таким образом, вектор "Германия" будет ближе к вектору "Франция" например, а "понедельник" будет ближе к вектору "вторник" 
	но, конечно, слова никогда не появляются изолированно, и до сих пор мы действительно не думали о том, что происходит с векторными представлениями, когда мы хотим представить конкретные короткие подфразы 
	что, если у меня есть две фразы: "страна моего рождения" и "место, где я родился" например 
	если бы я должен был построить систему обобщения текста, я бы не хотел, чтобы оба они были вместе, несмотря на то, что страна и место не были похожи и рождение как существительное и "где я родился" как глагол (?)
	но в идеале, если мы построим систему обобщения текста, мы не будем включать обе эти фразы правильно 
	и поэтому вопрос в том, как мы должны представлять значение этих более длинных фраз 
	и ответ, (...) сегодня заключается в отображении этих фраз в то же самое векторное пространство, где у нас были слова
	
	despite - несмотря на

4 (12) Семантические векторные пространства
	73
	00:02:46,069 --> 00:02:47,900

	и так, просто чтобы дать вам более широкое представление о том, куда это нас ставит, 
	есть много представлений для векторов одного слова 
	мы просматриваем векторы word2vec и Club, в основном они являются своего рода распределенными и распределительными методами
	это на самом деле много других, таких как Броун-кластеры, которые мы не собираемся охватывать много в этом курсе 
	но в основном они также захватывают основную текущую статистику, и поэтому они хороши, но они действительно не(?) могут захватывать более длинные фразы 
	и тогда есть много других моделей, которые очень широко используются, такие как модель мешок слов или модель на основе PCA, что мы обнаружили для представления документов
	и они достаточно хороши для поиска информации 
	мы просто хотим найти конкретные документы и упоминания конкретные фразы или исследования документов 
	мы просто хотим знать эти документы примерно о политике или спорте или подобных вещах 
	но, конечно, они игнорируют порядок слов  
	и поэтому мы не можем получить слишком подробное понимание внутри нашего представления 
	Итак, сегодня мы рассмотрим другой метод поиска векторов, которые действительно представляют фразы и предложения, не игнорируя порядок слов и фактически пытаясь захватить как синтаксическую структуру, так и семантическую информацию этих фраз

5 (13) как мы должны отображать фразы на векторное пространство?
	106
	00:04:01,219 --> 00:04:03,109

	Итак, это приводит нас к вопросу о том, как мы должны это делать, 
	и в этой лекции мы введем и будем использовать принцип композиционности, который в нашем случае утверждает, что смысловой вектор предложения определяется одним значением его слов - векторами слов, 
	и на протяжении всей лекции я буду описывать их, как буд-то они находятся всего в двух измерениях, но, конечно, как правило эта размерность будет от 21 до 300, что гораздо более размерно 
	и правил, которые используются для их объединения, 
	и в отличие от того, где мы просто идем слева направо, и мы просто вычисляем один вектор для всего, что мы читали так далеко от левой стороны, 
	на этот раз мы будем придерживаться грамматической структуры предложения, 
	и мы попытаемся найти конкретные именные фразы, которые идут вместе, 
	так, например, "мое рождение" сама по себе разумная грамматическая или синтаксическая фраза и аналогично "страна" - это фраза-существительное, 
	и поэтому мы можем объединить эти два, 
	а затем "мое рождение" - на самом деле является предложной фразой, 
	Так что вектор-слова "of" будет объединен со вектор-словом "мое рождение" и так далее, 
	а затем в идеале в конце его конечный результирующий вектор будет на самом деле где-то близко к другим странам в одном векторном пространстве слов, 
	и поэтому хорошая вещь об этих моделях сегодня заключается в том, что они в основном могут совместно изучать эти так называемые деревья синтаксического анализа и композиционные векторные представления, которые идеально захватывают синтаксическую и семантическую информацию фраз под ними и не из всего контекста, который у нас был до сих пор слева

6 (14) Разбор предложения: что мы хотим
	148
	00:05:44,479 --> 00:05:46,550

	так что же такое синтаксический анализ предложений 
	что, когда мы говорим, что хотим разобрать предложение, что мы на самом деле хотим от него 
	предположим, мы начинаем с фразы типа "кошка сидела на коврике" - очень простая фраза 
	что есть предложение?, что мы хотели бы иметь? - модель, которая понимает, что "кошка" является фразой-существительным 
	и теперь я могу заменить эту фразу-существительное любой другой фразой-существительным, И это все равно будет грамматическое английское предложение 
	поэтому я мог сказать: "машина сидела на коврике" или "весь класс сидел на коврике" 
	как и любой вид существительного, теперь можно заменить этим, и это все равно будет грамматическое предложение 
	аналогично "коврик" является фразой-существительным, А теперь "на коврике" является предложной фразой 
	и снова мы можем заменить эту предложную фразу любой другой и все равно получить синтаксически или грамматически правдоподобное английское предложение 
	таким образом, мы могли бы сказать "кошка сидела" за пределами этой лекции, которая не имеет никакого семантического смысла, но на самом деле является грамматически правильным предложением 
	и это большое важное различие правильная грамматика не говорит вам так много о семантике 
	а потом "сел на коврик" как глагольная фраза 
	и все это вместе - предложение. 
	это своего рода стандартный способ определения задач синтаксического анализа 
	мы получаем эти дискретные древовидные структуры, и они различны в зависимости от входных данных

7 (15) изучаем структуру и представление
	185
	00:07:10,430 --> 00:07:12,050

	конечно, что мы хотим, это не просто иметь эти дискретные структуры 
	но в идеале мы хотим, чтобы снова все было представлено в виде вектора 
	и поэтому мы рассмотрим, как мы можем вычислить эти векторные представления, которые теперь представляют любую конкретную подфразу без контекста
	дискретные древовидные структуры и они различны в зависимости от входных данных

8 (16) изучаем структуру и представление?
	192
	00:07:24,979 --> 00:07:28,160

	и поэтому первый вопрос, который мы должны задать себе здесь, - действительно ли нам нужны эти структуры 
	вы знаете, мы говорим, что мы изучаем структуру и представление 
	но прочитав последние пару лекций мы также узнали какую-то структуру и представление 
	так получилось, что это представление всегда было все, что слева от текущего слова

9 (17) Примечание: рекурсивные и рекуррентные нейронные сети
	202
	00:07:45,950 --> 00:07:50,210

	и это приводит нас к трем слайдам примечания о том, должны ли мы использовать эти рекурсивные структуры, древовидные структуры или просто изменять структуры, и это может быть очень длинным аргументом 
	Я собираюсь попробовать дистиллировать его всего за три слайда 
	но в основном основное различие здесь в том, что вы знаете, что цепи на самом деле специальные типы деревьев 
	правильно?
	они просто всегда ветвятся в одном направлении 
	правильно?
	Итак, из общей теории графов вы знаете, что изменения-это специальные типы деревьев 
	и поэтому в этом смысле рекурсивные сети - это просто обобщения рекуррентных 
	и по умолчанию может быть просто сказать, что я просто беру все слова и вместо того, чтобы смотреть на их грамматику, я просто объединяю их по одному 
	и поэтому вы можете видеть это, когда у вас есть эти конкретные слова, вы можете просто сказать, что я продолжаю комбинировать их по одному слову направо, и это также дерево 
	это просто случается, чтобы сгладить его и назвать его цепью 
	в этом смысле они на самом деле не так уж отличаются

10 (18) Примечание: являются ли языки рекурсивными?
	229
	00:08:58,880 --> 00:09:01,880

	однако вопрос в том, является ли язык рекурсивным
	и поэтому здесь длинная история статей
	и я не думаю, что еще ясно, является ли это когнитивно правдоподобным 
	например, люди действительно складывают определенные фразы в своих мозгах и так далее 
	и, к счастью, мы сейчас не занимаемся когнитивными науками. 

	таким образом, мы можем положить это на стол и сказать, хорошо, может быть, может быть нет, но рекурсия явно полезна для описания естественного языка 

	очень полезно сказать, например, что "церковь, которая имеет хорошие окна" - это существительное (именная фраза)
	Я могу заменить эту именную фразу другой именной фразой 
	у них есть грамматическое предложение 
	и все же внутри этой именной фразы у нас есть относительное предложение "которая имеет хорошие окна" 
	и это само по себе также содержит именную фразу, а именно "хорошие окна" 
	и так, что от простого описания языка явно очень полезно (?)

	и теперь есть в основном четыре аргумента, которые мы используем сейчас 
	и в основном говорят, что это просто полезно для многих различных задач, с которыми мы сталкиваемся в обработке естественного языка 

	один из них - неоднозначность, поэтому здесь я покажу вам первые два дерева разбора, которые выглядят немного более реалистично, что вы действительно увидите, когда будете разбирать предложение 
	и вы видите здесь предложение вам нужно прочитать их слева направо, 
	так что здесь "он ест спагетти с ложкой" 
	а в других предложениях "он ест спагетти с мясом" 
	теперь то, что это говорит нам здесь, в основном у нас есть PE (?), который является личным местоимением, а личные местоимения - это именные фразы
	у нас есть глагольная фраза, и теперь вся эта фраза "ест спагетти" или фраза "ест спагетти с ложкой" - это глагольная фраза 
	"ест" - глагол, "спагетти" - существительное множественного числа 
	и теперь это интересная часть
	это так называемый ПФ или предложная фраза 
	и эта общая проблема - неопределенность, связанная с ПФ
	так что, если компьютер должен был прочитать предложение, и вы знаете, читает, "он ест спагетти с ложкой" против "он ест спагетти с мясом" 
	теперь, как мы можем знать, понимает ли компьютер, что когда я говорю "ложкой", я на самом деле имею в виду, что я изменяю способ, которым я ем спагетти?
	в этой ПФ должен прикрепляться к глагольной фразе, 
	когда я говорю, что он ест "спагетти с мясом", тогда как это - с чем? - также просто есть какая-то именная фраза внутри нее, 
	эта ПФ фактически прикрепляется к типу спагетти
	поэтому, если у нас есть такая структура, и мы делаем это явным, будет полезно прояснить эти два случая, и теперь модель может на самом деле, сказать нам, правильно ли она это сделала 
	и вы знаете, что теперь мы можем выполнять задачи вниз по течению, а также запрашивать, например, модель и спрашивать, дайте мне все разные способы, которыми кто-то может есть спагетти, может быть, вы сделали это вилкой, может быть, вы делаете вилкой и ложкой и так далее

11 (19) -------------------------------
	305
	00:12:10,640 --> 00:12:12,800

	есть ли вопросы по этой части? 
	Я на самом деле взял целых два семестра только синтаксиса и теории грамматики и тому подобное 
	и опять же, это похоже на безумное упрощение, здесь можно буквально провести целый год, просто говоря обо всех тонкостях того, как эти деревья создаются, поэтому немного когнитивная перегрузка в некотором роде 
	<вопрос>
	вопрос в том, действительно ли они полезны для семантического понимания по сравнению с просто грамматическим пониманием 
	и ответ здесь в некотором роде как 
	и многие лингвисты постулируют, что для того, чтобы понять предложение, вы сначала понимаете слова 
	тогда вы поймете, как складываются слова 
	и тогда вы знаете, что вы добираетесь до фактического значения этого предложения, и вы понимаете, что сначала вы добираетесь до смысла через структуру 
	и некоторые люди говорят, ну может нет
	так что это очень много для обсуждения 
	ясно, что здесь мы видим, что вы знаете именную фразу, Ну, нет, нет никакой именной фразы здесь внутри другой именной фразы, но здесь здесь это имеет место прямо с мясом, мясо - это именная фраза в спагетти, поэтому мы поняли больше о грамматической структуре здесь, 
	и, конечно, если бы мы, например, обучали ли студенты используют правильную грамматику, это кажется явно очень полезной моделью  (?)
	однако у нас есть лучшее понимание того, что модель на самом деле здесь <можно> понять, если прочитать эти фразы правильно 
	так едят спагетти ложкой и теперь вот модель поняла, что с ложкой разобрались 

	понял, что с ложкой здесь на самом деле прикрепляется к глагольной фразе и, следовательно, она изменяет, как вы едите спагетти 
	таким образом, мы получили как синтаксическое, так и семантическое понимание этого 
	<вопрос>
	это отличный вопрос, и мы на самом деле доберемся до этого немного на протяжении всей лекции 

11 (19) Полезна ли рекурсия?
	358
	00:14:59,550 --> 00:15:02,759

	Итак, следующие три причины: скажем 
	рекурсивная структура когнитивно сомнительна 
	но явно полезно как способ описания языка 
	теперь это также полезно для реальных задач 
	Итак, предположим, что мы хотели, например, сделать несколько сложный совместный анализ ссылок, где и корефф, что мы в основном хотим понять, когда вы ссылаетесь сегодня или это 
	что ты имел в виду, когда сказал это? 
	верно?
	и это обычно означает, что у вас есть анафора, например 
	вот пример: "Джон и Джейн отправились на Большой фестиваль. они наслаждались поездкой и музыкой там"  
	теперь совместное разрешение ссылок будет задачей понять, кто они, когда я на самом деле упомянул, что в этот день прямо там 
	и в этом случае там были бы Джон и Джейн 
	Итак, теперь мы хотим обратиться ко всей этой фразе 
	теперь, когда я говорю "им нравится поездка", и я спрашиваю, что вы подразумеваете под "поездкой" 
	тогда в идеале вы бы сказали, что поездка означала "они пошли на фестиваль" 
	верно?
	так вот, теперь "поход на фестиваль" - это особый вид путешествия 
	и дело не в том, что "Джейн и Джон пошли на Большой фестиваль" - это конкретный тип поездки, это просто фестиваль 
	поэтому вы можете обратиться именно к этому, и теперь у кого-то другого может быть такая же поездка, и вы можете обратиться к этому как к одной когерентной единице 
	и теперь потенциально будет большой фестиваль, может быть, есть два фестиваля Большой в маленьком, они пошли на один, а не на другой, теперь вы идете в основном туда 
	поэтому в этом году в основном мы видели пару раз (?)
	мы могли бы также ссылаться только на нее и только на него, например, только на Джейн и только на Джона, и в обоих случаях мы не хотели бы иметь только представление обо всем, что читалось до сих пор 
	верно?
	мы только хотели бы иметь то, что мы можем сослаться на то, что является под-фразой в этом предложении 

	третья причина заключается в том, что маркировка иногда становится менее ясной 
	поэтому, если у нас в основном есть только одна метка в каждом слове
	Итак, здесь у нас есть предложение, такое как "мне нравится яркий экран, но не глючная медленная клавиатура телефона" 
	теперь в идеале мы могли бы просто классифицировать глючную клавиатуру телефона или глючную медленную клавиатуру сама по себе как сущность, и это отрицательно, тогда как что-то здесь есть положительное 
	верно?
	и мы не просто хотим классифицировать все предложение здесь или весь этот абзац 
	но мы идеально понимаем каждую из этих фраз, и они могут иметь разные типы ярлыков, если мы должны были классифицировать чувства, например 
	и снова здесь интересная проблема со ссылкой 
	было больно печатать или было приятно смотреть в зависимости от того, какая информация здесь является семантикой того, что следует за ней, на самом деле это либо относится к экрану, либо к клавиатуре 
	так что это задача коронера разрешения это на самом деле своего рода интересная задача для НЛП 
	хорошо? 

	и тогда последний аргумент является несколько прагматическим аргументом, который в некоторых задачах просто работает: лучше использовать эти грамматические структуры 
	но это также постоянная область исследований, поэтому, возможно, в конечном итоге мы обнаружим, что мы можем уйти, что является очень глубокой моделью lstm, и нам они вообще не нужны 
	и, может быть, всякий раз, когда у вас есть фраза, у вас могут быть некоторые нейроны, которые просто захватывают сейчас как фразу, а затем включается forget gate, и они являются следующим слоем, который будет иметь дело с этой вещью 
	это все еще обсуждается и это очень активная область исследования 
	но, хорошо, давайте пока предположим, что это все еще так, что на некоторых задачах эти модели работают лучше всего на некоторых стандартных эталонных наборах данных

12 (20) рекурсивная нейронная сеть для предсказания структуры
	450
	00:18:41,730 --> 00:18:43,740

	Итак, давайте определим, что такое рекурсивная нейронная сеть и как мы получаем эти древовидные структуры синтаксического анализа 
	таким образом, в основном у нас будет два входа, которые в целом являются представлениями детей-кандидатов 
	таким образом, вначале это будут просто слова, например "The" и "mat", а затем позже это могут быть представления векторов фраз, которые на самом деле уже были вычислены ранее 
	и выход в первом примере, где мы фактически смотрим на то, как вычислить древовидную структуру, будет двумя вещами 
	первый - семантическое представление, если мы объединим эти два узла 
	и второй будет оценка того, насколько правдоподобным будет новый узел 
	поэтому в идеале здесь мы снова вычислим счет 

13 (21) Recursive Neural Network Definition
	469
	00:19:27,530 --> 00:19:30,570

	который скажет, что это разумная синтаксическая фраза 
	и мы хотим увеличить этот счет, если это было действительно так, и мы хотим уменьшить его, если это какая-то случайная безграмотная фраза 
	так что уравнения для этой модели здесь 
	ну, это очень просто: первое, что мы делаем здесь, мы просто объединяем c1 и c2, дети, левый ребенок и правый ребенок каждого узла в дереве 
	мы объединяем их, а затем умножаем их на эту матрицу W здесь 
	и теперь родительский вектор P должен иметь ту же размерность, что и каждый из отдельных потомков 
	поэтому позвольте мне спросить вас, какой должна быть размерность W 
	предположим, что C1 и c2 являются N-мерными. это n-мерный вектор, теперь какова должна быть размерность W? 
	Говорите громче 
	да 
	ровно N на 2N 
	и чтобы понять это немного, мы также можем написать это по-другому, вместо того, чтобы писать W,  c1 и c2 
	мы могли бы переписать это и сказать, что W-блок из w1 w2, так как мы сказали, что это матрица R n на 2N 
	и эта матрица будет именно такой, и мы имеем здесь C1 и C2 в качестве нашего векторного представления 
	и теперь это фактически равно W1 C1 плюс W2 C2 
	правильно?
	и это очень похоже на различные уравнения, которые мы имели в рекуррентных нейронных сетях, где один был историей того, что у нас было в прошлом H_{t-1} , а другой - X_1 - следующее слово 
	теперь просто выясняется, что C может быть как одним словом, так и скрытыми измерениями 
	и мы рассмотрим пример здесь в следующих нескольких слайдах 
	поэтому причина, по которой мы называем это рекурсивным и перед рекуррентным, заключается в том, что мы используем также здесь одни и те же параметры W во всех узлах дерева 
	поэтому, чтобы вычислить любой из них здесь, мы разделим вес 
	поэтому мы связываем веса и в основном имеем тот же W здесь, чтобы вычислить каждый родительский вектор 
	да 
	это правильно 
	угу 
	слово эта модель не очень зависит от порядка слов 
	окей 
	Итак, это основное уравнение, и снова мы видим, что это действительно по существу просто стандартная однослойная нейронная сеть 
	просто случается, что в качестве входных данных два вектора, которые мы объединяем, которые модель действительно не знает 
	таким образом, модель - это всего лишь один вектор, но мы собираем их вместе, беря один вектор от левого дочернего элемента и один вектор от правого дочернего элемента 
	Итак, это центральное уравнение сегодняшнего дня

14 (22) Разбор предложения с RNN
	532
	00:22:48,350 --> 00:22:51,740

	Итак, как мы на самом деле используем это для вычисления древовидной структуры, которая захватывает грамматику этого предложения 
	мы в основном, и это первая простая версия того, как описать это и в основном единственный, который мы подробно опишем сегодня так называемый механизм жадного поиска здесь 
	мы просто в основном смотрим, насколько вероятно было бы объединить вектор "THE" и вектор "cat" с этой нейронной сетью 
	и сначала это будет какое-то векторное представление, а затем оценка, которая будет просто линейным слоем 
	Итак, снова здесь оценка, которую мы пропустили, - это просто простое скалярное произведение, такой же счет, который мы использовали раньше 
	Итак, теперь мы рассмотрим это, а затем мы просто рассмотрим каждую возможную пару слов и насколько вероятно, что они будут объединены в разумную синтаксическую фразу 
	а сейчас это как раз в граматической структуре и по мнению лингвистов на то не разумная синтаксическая фраза 
	но "the mat" - это хорошая именная фраза, так что вы получите большой балл 
	и точно так же "the cat" является очень очевидной очень простой комбинацией определителя существительного, поэтому мы складываем их вместе

15 (23) разбор предложения
	560
	00:24:02,840 --> 00:24:05,660

	так что теперь "the cat" здесь получает более высокий балл 
	и в этом жадном поисковом механизме мы просто скажем: хорошо, хорошо, тогда давайте объединим эти два 
	давайте вместо этих двух отдельных векторов слов здесь мы теперь просто возьмем вектор фразы, который на самом деле является только выходом скрытого слоя 
	Итак, теперь модель в основном имеет один вектор здесь, и это "первый вектор" последовательности, и этот вектор представляет теперь "the cat" 
	и теперь у нас в основном есть здесь "the cat" имеет вектор, а затем все еще "sat on the mat" , поскольку каждый из них является отдельным вектором слов 
	и теперь модель еще не знает, насколько вероятно было бы совместить "the cat sat" 
	и вот где рекурсия приходит на выходе в первый раз, когда мы применяем ее, нейронная сеть становится входом в ту же самую нейронную сеть, и именно поэтому она рекурсивна 
	так что теперь "the cat sat" - это несколько разумная фраза, которую мы могли бы просто сказать "the cat sat", а не если "the cat" останется 
	и так это получило бы разумно большой счет 
	однако "the mat" был еще более очевидной именной фразой

16 (24) разбор предложения
	588
	00:25:21,900 --> 00:25:26,280

	и поэтому мы объединим эти два узла 
	и теперь снова модель не знает, насколько вероятно было бы объединить вектор слова "on" с вектором фразы "the mat" 
	и поэтому мы снова применим ту же нейронную сеть 
	да 
	это отличный вопрос 
	это заходит слишком далеко, поэтому я не могу вдаваться в подробности. 
	но для тех из вас, кто на самом деле взял до 24 и нет CQ (?) я вижу, что вы в порядке, это всего лишь три имени разбора и диаграммы разбора 
	в анализе диаграмм вы можете сделать определенные предположения о независимости, как вы делаете предположения Маркова для языковых моделей 
	мы говорим: Мне все равно, что было раньше, но сейчас эта фраза является именной фразой 
	и из-за этого вы можете сказать: Ну, я могу найти глобальный оптимум здесь 
	поскольку я могу сделать некоторые упрощающие предположения, используйте динамическое программирование здесь, вы не можете действительно использовать это, потому что векторы, на которых вы делаете эти предположения, непрерывны 
	и поэтому действительно изменение вектора только немного изменит потенциально общий балл 
	таким образом, хотя одна фраза может быть локально лучшей фразой с самым высоким баллом, на самом деле она может не соответствовать глобальному оптимуму 
	таким образом, все, что вы могли бы сделать здесь, это так называемый динамический поиск луча, но не волнуйтесь, если вы не смогли следовать этой маленькой заметки 
	так что в конце концов, что мы будем делать, это CKY, как поиск луча, но опять же не беспокойтесь об этом, этого достаточно, чтобы понять 
	так что эта жадная процедура здесь для поиска лучшего дерева

17 (25) разбор предложения
	626
	00:27:08,679 --> 00:27:10,659

	Итак, теперь мы в основном продолжаем строить это дерево, находим потенциального кандидата с самым высоким баллом и, в конце концов, имеем полную древовидную структуру 
	да 
	прости, ты можешь говорить немного громче? 
	вот так есть определенные ограничения на эти два, что они должны быть симметричными 
	это на самом деле интересный вопрос 
	их нет, поэтому прямо сейчас они могут быть любой произвольной матрицей 
	и оказывается, что есть интересные расширения, которые мы рассмотрим позже в лекции, которые на самом деле позволяют нам действительно понять немного лучше, как каждый, какова роль этих двух блоков, является их левым блоком и правым блоком 
	ну, мы доберемся до него через пару слайдов 
	Но да, в общем, это просто W-матрица, она будет обновлена SGD или другими методами оптимизации, и нет никакого способа сортировать силу и силу, которые вы не хотите применять симметрию, потому что, возможно, слева левая фраза или слово может быть более важным, чем правая 
	и любые другие вопросы о том, как такая жадная процедура поиска произошла 

	и все же да. 
	одна и та же нейронная сеть здесь использовалась для вычисления каждого отдельного узла 

	ладно 
	так что вопрос я думаю, что это немного не нормально 
	для этого хорошо получается, что это нормально для задач для определенных задач 
	угу 
	это достаточно хорошо, чтобы получить эти день перформанса на них 
	угу 
	поэтому вопрос в том, хорошо ли иметь разные виды нейронных сетей и фактически расслаблять предположения 
	так что, как я описываю это до сих пор, эти типы моделей на самом деле более рекурсивны, чем фактический естественный язык 
	тесты естественного языка иногда именная фраза внутри относительного предложения внутри именной фразы 
	здесь мы используем одну и ту же нейронную сеть в каждом узле дерева 
	и поэтому мы действительно ослабим это предположение позже на лекции

18 (26) Max-Margin Framework - Детали
	679
	00:30:10,800 --> 00:30:12,720

	так как мы на самом деле тренируем это?
	мы будем использовать аналогичную целевую функцию max margin, которую мы использовали ранее в одной из предыдущих лекций 
	но теперь мы определим область действия всего дерева 
	и у нас не просто одно окно, у нас есть дерево, которое на самом деле было вычислено из нескольких разных видов оценок 
	и поэтому оценка полного дерева для нас сейчас просто сумма всех баллов в каждом узле 
	Итак, давайте определим здесь оценку конкретного предложения X с определенным деревом 
	мы в основном смотрим на все узлы, которые были у дерева, и вычисляем оценки на каждом из этих узлов 
	и снова эти оценки здесь были просто простыми скалярными произведениями с тем же вектором представления

19 (27) Max-Margin Framework - Детали
	697
	00:31:01,710 --> 00:31:03,240

	so here comes the the most important and kind of interesting new equation of today's lecture 
	basically this is called max margin parsing 
	and we have here this max margin objective function 
	so let's walk through this objective function very slowly

	what we want to do? we want to generally maximize this function 
	and we assume we have a labeled training data set 
	so we assume a bunch of linguists sat down and said for every sentence X_I I have I give you the correct tree W_I 
	and now this again here was just the sum of all the nodes of those trees and that sum was just a sum of a bunch of inner products 
	right?
	so again 
	when you think about how would you train this model 
	it's just a bunch of gradients of a sum of inner products underneath which you have a neural network 

	okay so that's easy 
	but now if we just maximize all the scores of all the correct trees we could just everything would go to infinity, we just maximize everything 
	well 
	we actually have to tell the model is how to deal with finding the right tree by itself 
	and this is what the second part here is 
	so we're going to try to find the maximum over this set and the set "A" of X_I is the set of all possible trees that you could construct from X_I 
	and so there if you're familiar with with combinatorics they're Catalan many potential binary trees 
	so it's exponentially many possible binary trees you can actually compute that number and blows up very very quickly 
	it's an exponentially many number of trees 
	so this is where we will have to find some smarter way to go through them and find the highest-scoring one 
	and the simplest one that I described was in basically this greedy procedure 
	here we just take the highest-scoring current set 
	and hope that that will lead us eventually to the highest-scoring tree 
	but of course that isn't necessarily true 
	here we found the correct tree but maybe the model incorrectly would have said "cat sat" is the highest-scoring one 
	and then we couldn't recover that later on 
	right?

	so this is basically a search procedure and they're different ways to do search 
	and in some ways if you interested in kind of the search aspect of machine learning and they I then I think 221 is the right kind of lecture takes sort of an introduction to AI different kind of search strategies 
	all right 

	so let's assume for now that we find the highest-scoring one simply with this greedy procedure 
	and we basically you know do exactly what I described the previous slides and now we define one specific Y that is the maximum of all the trees which we could build 
	and now because of this my here we're going to try to minimize the score of that highest-scoring tree 
	and let's assume let's ignore this part for now if the highest-scoring tree that we're now minimizing actually happens to be the exact correct tree 
	well then we're done here 
	right then why I and this y are actually the same these two cancel out and we're done 
	and now this is where the interesting margin penalty comes in which as indeed actually an important part of this objective function 
	and this margin here essentially penalizes every incorrect decision that you have made 
	so if you incorrectly combine two words then you add one to that tree structure and what that the result of that will be that once you actually get the correct tree here as Y 
	and y actually is weii then that Y will have a high score that is higher up to a certain margin to the next scoring but incorrect tree 
	let's parse that 
	pretty complex sentence 
	so essentially what this Delta here does is it encourages the model to make mistakes 
	it will add a bonus point to every mistake the model makes and once it goes past that bonus it actually makes the right decision with a margin of Delta which you usually just for every incorrect decision we set it to one 
	so to score for the right decision will that new one larger than the score of an incorrect decision 
	so in the beginning here we encourage the model to basically do the worst possible thing 
	all right and then model makes all these mistakes and then you can tell them well these were all wrong and you minimize all the scores of these incorrect decisions 
	and that's that's where yeah that's sort of the idea of the next margin loss 
	are there any questions about this 
	yeah 
	so in almost every single weight in all these different loss functions that I always describe has a standard l2 penalty and all the weights 
	the Delta yeah the Delta does not it just basically encourages the model to make mistakes during the search procedure 
	it doesn't actually back up something in itself 
	great question 
	yes 
	I okay so again X I is the sentence 
	so in this case you know your cat sat on the mat Y is the correct tree structure basically linguists sat down went through a lot of sentences in most cases so-called penn treebank and in the pantry bank they said all right for this sentence this is the correct grammatical analysis of that sentence 
	all right 
	yes 
	can one turn be negative 
	yes so these scores here just inner products right inner products of real numbers so they can be very negative 

